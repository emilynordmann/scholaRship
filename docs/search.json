[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"scholaRship collection tutorials engage scholarship teaching learning using quantitative methods R. written Learning, Teaching, Scholarship focused academics: Emily Nordmann, James Bartlett Gaby Mahrholz School Psychology Neuroscience, University Glasgow, Mitchum Bock, Eilidh Jack, Craig Alexander, School Mathematics Statistics, University Glasgow, Jill MacKay, University Edinburgh.book considered living document. tutorials added time developed content structure new existing tutorials may updated.Contact: Emily Nordmann (Emily.Nordmann@glasgow.ac.uk)","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"Getting started scholarship teaching learning can difficult. majority academics whose subject expertise involve learning teaching, first hurdle figuring questions can ask answer (indeed interested ) can toughest one push past.settled area inquiry, may find appropriate methodologies investigate questions ones trained . quantitatively-minded researchers, availability data can feel like simultaneous feast famine - may access huge amounts data learning analytics standard student records able use almost none research purposes due need opt-consent. consent obtained, may small, non-representative samples, non-random attrition, /concerns making data openly available.Finally, data can seriously messy: missing data, data multiple sources different structures labels, data different academic years course structures assessments changed, anonymised data, aggregated data.sounds familiar, book .tutorial book contain:short summary evidence-base problem investigation promote engagement SoTL literature;Real1 data drawn commonly available sources Moodle, Turnitin, Microsoft Forms, Echo360;walkthrough clean wrangle data using predominantly tidyverse approach;walkthrough analyse interpret, analysis, alongside honest discussion limitations approach used.","code":""},{"path":"intro.html","id":"planned-tutorials","chapter":"1 Introduction","heading":"1.1 Planned tutorials","text":"following tutorials planned happy take suggestions - please e-mail Emily.Nordmann@glasgow.ac.uk.Analysing impact whether students check feedback Turnitin Feedback Studio subsequent assessment performance.Creating exam board moderation reports using RUsing Moodle logs predict engagement retention","code":""},{"path":"intro.html","id":"expectations-of-prior-knowledge","chapter":"1 Introduction","heading":"1.2 Expectations of prior knowledge","text":"","code":""},{"path":"intro.html","id":"r-and-rstudio","chapter":"1 Introduction","heading":"1.2.1 R and RStudio","text":"Minimal prior knowledge R RStudio assumed throughout book. functions code used explained, however, assume reader understands :Install R RStudioInstall R RStudioNavigate RStudioNavigate RStudioSet working directory appropriatelySet working directory appropriatelyInstall load packagesInstall load packagesWrite execute codeWrite execute codeEach chapter start overview expected prior knowledge along resources recap necessary.","code":""},{"path":"intro.html","id":"research-methods-and-statistics","chapter":"1 Introduction","heading":"1.2.2 Research methods and statistics","text":"assume basic level competency research methods statistics. However, also recognise many researchers still less familiar modern approaches mixed effects models provide appropriate level explanation resources necessary.","code":""},{"path":"creating-synthetic-datasets.html","id":"creating-synthetic-datasets","chapter":"2 Creating synthetic datasets","heading":"2 Creating synthetic datasets","text":"Lead author: James Bartlett","code":""},{"path":"creating-synthetic-datasets.html","id":"introduction","chapter":"2 Creating synthetic datasets","heading":"2.1 Introduction","text":"scholarship teaching learning, might analyse data sources like virtual learning environments student characteristics. Student agreements normally allow educators analyse data purposes internally improving education experiences, want disseminate findings wider audience, need gain ethical approval students use data additional purposes. Even ethical approval, might additional concerns around privacy anonymity data contain sensitive personal information. data sharing practices become routine, important contribute scholarly progress maintaining participant anonymity. tutorial, demonstrate can create synthetic data sets strategy sharing primary data present ethical issues.Synthetic data mimics properties original data closely possible, retaining distribution grades sample. means can retain underlying statistical properties variables, \"individual participants\" longer represent original cases, meaning risk identification much lower. R package synthpop (Nowok et al., 2016) creates synthetic data sampling probability distribution best suited variables data set. precise technical details beyond scope tutorial, refer interested readers Nowok et al. information.important evaluate project individually judge whether can ethically share research data, Meyer (2018) provides practical tips:Wherever possible, get informed consent participants retain share data.Wherever possible, get informed consent participants retain share data.promise destroy never share data. Often researchers reassure ethical review boards data never shared beyond research team, providing appropriate , can simply ask participants consent data shared.promise destroy never share data. Often researchers reassure ethical review boards data never shared beyond research team, providing appropriate , can simply ask participants consent data shared.thoughtful considering risks re-identification.thoughtful considering risks re-identification.Assuming receive consent data sharing, likelihood identifying individual participants motivates synthetic data tutorial. Even exclude identifiable information like names email addresses, participants re-identified combining common demographic variables ethnicity, date birth, sex might want share related research question. risk re-identification may even higher participants know . example, students might know studied cohort able identify data set recognise participants relative details. Therefore, want share data part scholarship receive consent participants , important think whether variables want share used identify participants whether providing synthetic data set original data appropriate. See Meyer (2018) details ethical data sharing.tutorial, demonstrate create synthetic data set using synthpop package R. Quintana (2020) previously written accessible tutorial package recommend focuses biomedical science data. tutorial use data student's academic procrastination resonate closely scholarship teaching learning.","code":""},{"path":"creating-synthetic-datasets.html","id":"dunn-2014-replication","chapter":"2 Creating synthetic datasets","heading":"2.1.1 Dunn (2014) Replication","text":"data set use unpublished replication attempt Dunn (2014). Dunn wanted understand impact motivation statistics anxiety students' academic procrastination. sample included graduate students online course. wanted replicate study see observe similar findings response COVID-19 pandemic face--face students forced study online. Dunn replication perfect demonstrate synthetic data set includes range data types included open data consent forms, meaning can openly compare properties data set.research question : intrinsic motivation, academic self-regulation, statistics anxiety influence students' passive procrastination? expected intrinsic motivation self-regulation negative predictors, whereas expected statistics anxiety positive predictor passive procrastination. study included following variables:General strategies learning (GSL; GSL) - modified subscale Motivated Strategies Learning Questionnaire (MSLQ) measures academic self-regulation. Measured 1-7 scale, higher values meaning higher self-regulation.General strategies learning (GSL; GSL) - modified subscale Motivated Strategies Learning Questionnaire (MSLQ) measures academic self-regulation. Measured 1-7 scale, higher values meaning higher self-regulation.Intrinsic motivation (Intrinsic) - subscale MSLQ measures intrinsic motivation inherent joy people find task. Measured 1-7 scale, higher values meaning higher intrinsic motivation.Intrinsic motivation (Intrinsic) - subscale MSLQ measures intrinsic motivation inherent joy people find task. Measured 1-7 scale, higher values meaning higher intrinsic motivation.Statistical Anxiety Rating Scale (STARS; STARS) - measure statistics anxiety included statistics test class anxiety subscale. Measured 1-5 scale, higher values meaning higher statistics anxiety.Statistical Anxiety Rating Scale (STARS; STARS) - measure statistics anxiety included statistics test class anxiety subscale. Measured 1-5 scale, higher values meaning higher statistics anxiety.Procrastination Assessment Scale Students (PASS; PASS) - measure passive procrastination keeping writing assignments studying exams. Measured 1-5 scale higher scores meaning greater procrastination, scale uses sum items creating possible range 6-30.Procrastination Assessment Scale Students (PASS; PASS) - measure passive procrastination keeping writing assignments studying exams. Measured 1-5 scale higher scores meaning greater procrastination, scale uses sum items creating possible range 6-30.","code":""},{"path":"creating-synthetic-datasets.html","id":"prior-knowledge","chapter":"2 Creating synthetic datasets","heading":"2.1.2 Prior knowledge","text":"complete tutorial need following prior knowledge - recap links point additional materials cover detail):install load packages R (recap)set working directory run code R (recap)() use pipe %>% operator (recap)conceptual understanding linear regression (recap)","code":""},{"path":"creating-synthetic-datasets.html","id":"synthpop-setup","chapter":"2 Creating synthetic datasets","heading":"2.1.3 Set-up","text":"demonstrate synthpop package, explore original data set modelling.Download Dunn-replication.csv save folder named data working directory.run code load required packages (may require installation) data.","code":"\nlibrary(tidyverse) # Collection of functions for data wrangling and visualisation\nlibrary(performance) # Functions for assessing statistical models\nlibrary(effectsize) # Functions for calculating effect sizes\nlibrary(synthpop) # Package to create our synthetic data set later\n\n# Two functions to make prettier html tables\nlibrary(knitr)\nlibrary(kableExtra)\n\nreal_data <- read_csv(\"data/Dunn-replication.csv\") %>% \n  select(-user_id, -SelfEfficacy, -HelpSeeking) # omit some columns we don't need"},{"path":"creating-synthetic-datasets.html","id":"original-dataset-analyses","chapter":"2 Creating synthetic datasets","heading":"2.2 Original dataset analyses","text":"address research question using model Dunn (2014), want use linear regression PASS outcome three predictors GSL, intrinsic motivation, statistics anxiety.function lm() constructs linear modelThe formula takes form outcome ~ predictor1 + predictor2 etc.first save results regression object model pass object summary() function explore model results:replication different setting, results pretty close Dunn (2014). Overall, significant model explains 18% (adj. \\(R^2\\) = 0.16) variance passive procrastination. Similar Dunn, GSL significant negative predictor passive procrastination, 1 unit increase GSL associated 1.77 decrease passive procrastination. Intrinsic motivation non-significant weak negative predictor. Interestingly, statistics anxiety significant predictor Dunn, significant positive predictor replication, 1 unit increase STARS associated 1.69 increase passive procrastination. words, passive procrastination tended increase higher levels statistics anxiety lower levels GSL.values original units measurement, can also report standardised coefficients express values standard deviations, consistent Dunn reported results. can standardising predictors entering model, can use handy standardize_parameters() function effectsize package get 95% confidence intervals . , save results table add estimates Dunn (2014) comparison.code:Calculates standardized coefficients saves object named tableCreates vector coefficients original Dunn studyAdds vector column tableMakes nice looking table using kable - note table output might look slightly different bookdown package use write book applies additional formatting.\nTable 2.1: Standardised Beta coefficients 95% confidence interval (CI) replication data compared Dunn (2014).\ncomparison, coefficient STARS larger Dunn, coefficients GSL intrinsic motivation smaller. findings largely consistent though, coefficients direction within 95% CI estimates. Despite intrinsic motivation significant predictor, findings consistent hypotheses largely replicate findings Dunn.final check, important make sure model results consistent assumptions linear regression. great package called performance includes helper functions like check_model(). can concisely check model assumptions:Although bottom right plot normality residuals quite capturing peak normal distribution, nothing suggest warning signs model.findings assumptions order, know real data set telling us. Now, time create synthetic data set using synthpop package see close replicates features.","code":"\n# One outcome, three predictors\nmodel <- lm(PASS ~ GSL + Intrinsic + STARS,\n            data = real_data)\n\n(model_results <- summary(model)) # surround with brackets to both print and save## \n## Call:\n## lm(formula = PASS ~ GSL + Intrinsic + STARS, data = real_data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -14.8588  -3.4830   0.0803   3.2853   9.9100 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  23.6386     3.0068   7.862  3.2e-12 ***\n## GSL          -1.7677     0.4698  -3.762 0.000275 ***\n## Intrinsic    -0.1780     0.5160  -0.345 0.730767    \n## STARS         1.6926     0.5961   2.840 0.005408 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.405 on 107 degrees of freedom\n## Multiple R-squared:  0.1815, Adjusted R-squared:  0.1586 \n## F-statistic: 7.911 on 3 and 107 DF,  p-value: 8.154e-05\n#1. Use the standardize_parameters function\n#2. Select rows 2-4, ignoring the intercept\n#3. Save in a data frame\n#4. Drop the 95% CI column as it's just descriptive \n\ntable <- standardize_parameters(model) %>%\n  slice(2:7) %>%\n  data.frame() %>%\n  select(-CI)\n\n# Manually save values from the original Dunn study for comparison\ndunn <- c(-0.55, # GSL\n          -0.17, # Intrinsic\n          0.15) # STARS\n\n# Add these values to our table above\ntable$dunn <- dunn\n\n# Use the kable and kableextra functions to create a nicer looking table \nkable(table, \n      digits = 2, \n      format = \"html\", \n      col.names = c(\"Parameter\", \"Beta\", \"Lower 95% CI\", \"Upper 95% CI\", \"Beta from Dunn (2014)\"), \n      caption = \"Standardised Beta coefficients and their 95% confidence interval (CI) in the replication data compared to Dunn (2014).\") %>% \n  kable_styling()\ncheck_model(model)"},{"path":"creating-synthetic-datasets.html","id":"synthesise-with-synthpop","chapter":"2 Creating synthetic datasets","heading":"2.3 Synthesise with synthpop","text":"synthpop package (Nowok et al., 2016) aims mimic observed data preserve relationship variables. authors developed package work around limitations working vast data coming national statistical agencies. population level data sets can provide important insights, granularity data rightly leads privacy concerns identifiable participants , restricting access data. Working higher education data, can face similar concerns. access student level data can provide important insights, often access share data confidentiality constraints. synthetic data can useful.Packages like synthpop attempt reconstruct data set sampling probability distributions relevant type data working . means properties variables relationships retained closely possible. Sometimes less accurate, explore factors keep mind later .","code":""},{"path":"creating-synthetic-datasets.html","id":"preparing-the-data-set","chapter":"2 Creating synthetic datasets","heading":"2.3.1 Preparing the data set","text":"creating synthetic data set important check data determine number type variables, well much missing data () can using codebook.syn() function.Whilst output might look useful, actually missing lot information dataset individual variables quite right format.first step convert object data frame. used \"tidyverse\" family packages, data saved something called tibble. Tibbles similar data frames, tweaks work better tidyverse (see tibble chapter R Data Science book; Wickham & Grolemund, 2017). Although tibbles can useful within tidyverse framework, can occasionally cause problems. found hard way writing tutorial try use data without converting data frame, get cryptic error message Error tab.obs[[]] + tab.syn[[]] : non-conformable array took us hour figure . reading forum highlighting tibbles caused similar problem another package tried converting data frame first solved issue.second step ensure character variables set factors whilst data columns text data, informative text represents different categories. creating synthetic data set, function works probability distribution using original data, setting characters factors establishes number unique groups per variable.Now, repeat codebook function, get added details informing us number levels per variable factors data range numeric variables. data set, missing data, , central columns inform us number percent missing values, also something can estimated part synthetic data function.","code":"\ncodebook.syn(real_data)## $tab\n##           variable     class nmiss perctmiss ndistinct details\n## 1              age   numeric     0         0        16        \n## 2           gender character     0         0         4        \n## 3 degree_programme character     0         0         4        \n## 4              GSL   numeric     0         0        24        \n## 5        Intrinsic   numeric     0         0        18        \n## 6            STARS   numeric     0         0        25        \n## 7             PASS   numeric     0         0        20        \n## \n## $labs\n## NULL\n# Resave real_data as a data frame instead of a tibble\nreal_data <- as.data.frame(real_data)\n\n# Convert gender to a factor\nreal_data$gender <- as.factor(real_data$gender)\n\n# Convert degree programme to a factor\nreal_data$degree_programme <- as.factor(real_data$degree_programme)\ncodebook.syn(real_data)## $tab\n##           variable   class nmiss perctmiss ndistinct           details\n## 1              age numeric     0         0        16    Range: 18 - 50\n## 2           gender  factor     0         0         4 See table in labs\n## 3 degree_programme  factor     0         0         4 See table in labs\n## 4              GSL numeric     0         0        24  Range: 1.6 - 6.8\n## 5        Intrinsic numeric     0         0        18 Range: 2.5 - 6.75\n## 6            STARS numeric     0         0        25  Range: 1.875 - 5\n## 7             PASS numeric     0         0        20     Range: 6 - 30\n## \n## $labs\n## $labs$gender\n##                  label\n## 1               Female\n## 2                 Male\n## 3           Non-binary\n## 4 Prefer not to answer\n## \n## $labs$degree_programme\n##   label\n## 1    BA\n## 2   BSc\n## 3    MA\n## 4   MSc"},{"path":"creating-synthetic-datasets.html","id":"subset-the-data","chapter":"2 Creating synthetic datasets","heading":"2.3.2 Subset the data","text":"Previously, mentioned synthpop match features original data closely possible, can less accurate. One factor associated ratio number participants number variables data set. synthetic data set tries retain association variables, larger number variables, combinations function must consider. enough participants, synthpop provide warning number participants recommends based number variables data set. 100 + 10 * number variables. , two variables, recommend 120 participants, eight variables 180 participants etc. fewer participants recommended, estimation process might less precise.demonstration, first create limited data set two variables show works, scale full data set. following code, set seed make analyses reproducible. function set.seed() controls random number generator - using functions use randomness, running set.seed() ensure get result time run function (many cases may want ). estimation synthetic data set based simulations, set seed get results tutorial.know GSL strongest predictor original data, limit results just predicting academic procrastination GSL create synthetic data set using syn() function.Now, synthetic data set saved object environment, synthpop includes function save new data current working directory. following function takes synthetic data object, like file called, file type want saved :ran code save synthetic data set, see three new files working directory. . RData object can reload R. choosing .csv file, also spreadsheet containing data analyse software read R. Finally, .txt file information synthetic data process like name original data file, seed used, variables.","code":"\n# Set a seed for reproducible analyses\nmy_seed <- 2018\n\n# limit the real data to just two variables \nshort_data <- real_data %>% \n  select(PASS, GSL)\n\n# Save the synthetic data using the reproducible seed from above\nsynth_data <- syn(short_data, \n                   seed = my_seed)## CAUTION: Your data set has fewer observations (111) than we advise.\n## We suggest that there should be at least 120 observations\n## (100 + 10 * no. of variables used in modelling the data).\n## Please check your synthetic data carefully with functions\n## compare(), utility.tab(), and utility.gen().\n## \n## \n## Synthesis\n## -----------\n##  PASS GSL\nwrite.syn(synth_data, \n          filename = \"synthetic_Dunn_data\", \n          filetype = \"csv\")## Synthetic data exported as csv file(s).\n## Information on synthetic data written to\n##   C:/Users/staff/OneDrive - University of Glasgow/Teaching/psyteachR/scholaRship/book/synthesis_info_synthetic_Dunn_data.txt"},{"path":"creating-synthetic-datasets.html","id":"compare-data-sets","chapter":"2 Creating synthetic datasets","heading":"2.3.3 Compare data sets","text":"Now synthetic data, synthpop useful functions compare new data observed data. example, can compare distribution values two data sets see well reconstructed values. stat argument, can choose either \"counts\" \"percents\" show histogram frequency percentage values, depending prefer interpret.can see two histograms limited selection two variables. dark blue, frequency observations observed data, light blue, frequency observations synthetic data. perfect match, observed values higher synthetic, synthetic values higher observed. never going perfect, trying capture features closely possible, different underlying distributions can produce relationships variables.histograms, default settings utility measures. statistics summarise closely synthetic data compares observed data, assuming synthesis model correct. three default results pMSE (propensity score mean-squared error), S_pMSE (standardised measure pMSE), df (degrees freedom Chi-Square tests). many utility measures can request using utility.stats argument. explore measures available, read documentation entering help(\"utility.tab\") console.want fall statistics rabbit role, refer Raab et al. (2021) Snoke et al. (2018) test different utility measures. brief overview, general/global specific/narrow utility measures. Global measures attempt capture well synthesis process reconstructs relationships across whole data set rather result one specific statistical model. output , pMSE one measure. Propensity scores work combining two data sets calculating probability observation comes synthetic data. means propensity score mean-squared error (pMSE) measures error associated process, higher values meaning greater error. closer 0 better, means highest utility hard distinguish two data sets. away 0, easier distinguish two data sets. values small, nothing worry .hand, narrow measures compare models data sets, closely regression coefficients replicated. lm.synds() function synthpop creates linear model using synthetic data sets, can refit using observed data compare different .following code, first create simple linear regression model using PASS outcome GSL single predictor. save object use compare() function .bunch output dissect , start plot visual overview. Similar last compare() output, observed data displayed dark blue synthetic data displayed light blue. plot shows point estimate confidence interval regression coefficient Z value. Visually, looks pretty good. synthetic data estimate slightly smaller, confidence intervals largely overlap.Now initial impression, break output table. informing model fitted, summary model coefficients synthetic observed data, difference , much confidence intervals overlap. intercept, difference -0.46 GSL coefficient, difference 0.10. quite small, keep mind original units measurement, need interpret differences relative measures. Reinforcing visual interpretation, confidence intervals largely overlap, approximately 93% coverage parameters.Finally, Chi-Square test assumes synthetic data model compatible observed data model. essence, null hypothesis difference two models. Acknowledging limitations null hypothesis significance testing, smaller p-value, incompatible two models . p-value close 1, suggesting significant difference two models. p-value much smaller, traditional threshold .05, suspect synthetic data process capture relationships observed data.summarise smaller selection procedure, limited data set two variables: predicting PASS GSL scores. created synthetic data set using synthpop package explored general narrow utility measures. types measures showed synthetic data set good job capturing properties observed data. means share synthetic data set concerns sharing original observed data set. Just remember clearly label synthetic data set fake data set inform readers replaced observed data.","code":"\ncompare(synth_data, # synthetic data object\n        short_data, # original data\n        stat = \"counts\") # Choice of counts or percents## \n## Comparing counts observed with synthetic\n## \n## \n## Selected utility measures:\n##          pMSE   S_pMSE df\n## PASS 0.000207 0.091919  4\n## GSL  0.002052 0.911044  4\n# Linear model equivalent in synthetic data, one outcome and one predictor\ns_lm <- lm.synds(PASS ~ GSL,\n                 data = synth_data)\n\ncompare(s_lm, # saved object from above for lm applied to synthetic data\n        short_data) # original data## \n## Call used to fit models to the data:\n## lm.synds(formula = PASS ~ GSL, data = synth_data)\n## \n## Differences between results based on synthetic and observed data:\n##             Synthetic  Observed       Diff Std. coef diff CI overlap\n## (Intercept) 27.856804 28.319159 -0.4623549     -0.2619675  0.9331703\n## GSL         -1.497014 -1.598289  0.1012751      0.2409324  0.9385365\n## \n## Measures for one synthesis and 2 coefficients\n## Mean confidence interval overlap:  0.9358534\n## Mean absolute std. coef diff:  0.25145\n## \n## Mahalanobis distance ratio for lack-of-fit (target 1.0): 0.04\n## Lack-of-fit test: 0.07152817; p-value 0.9649 for test that synthesis model is\n## compatible with a chi-squared test with 2 degrees of freedom.\n## \n## Confidence interval plot:"},{"path":"creating-synthetic-datasets.html","id":"full-data-set","chapter":"2 Creating synthetic datasets","heading":"2.3.4 Full data set","text":"Now taken close look limited selection variables, scale things see synthpop looks like variable types relationships consider. go back using original real_data file seven variables. first step using syn() function create new synthetic data set. already processed data set reducing number variables, remember check using tibble whether character variables need converting factors.working amount data 111 participants trying reconstruct variables. Remember synthetic data process based sampling appropriate probability distribution, package recommends minimum number participants (100 + 10 * number predictors). smaller selection, close recommendation two predictors least 120 participants. However, larger selection, away recommendation now least 170 participants. Keep mind checking utility measures.Next, compare two data sets see close reconstructed variables:output longer since now seven variables reconstruct instead smaller selection two. can also see character variables look like since smaller selection contained two numeric variables. demographic information omitted smaller selection, can now see age, gender, participant's degree programme., variables interest use next multiple linear regression, using PASS (academic procrastination) outcome three predictors intrinsic motivation, GSL (general strategies learning), STARS (statistics test anxiety). final table general utility measures see well synthetic data set captured features observed data set. Remember pMSE error associated classifying data coming synthetic observed data set, values 0 indicating greater error.use function create linear model synthetic data set, use compare() function see model performs data set narrow utility measures:Starting visual interpretation, synthetic (light blue) observed (dark blue) estimates pretty close. GSL strong negative predictor, STARS moderate positive predictor, intrinsic motivation weak predictor PASS hovering around zero. confidence intervals overlap quite closely smaller selection, look precise statistics moment.Moving table estimates, now four parameters check. coefficients synthetic data set, observed data set, difference . Remember values unstandardised original units measurement, judge differences relative. Compared smaller selection, harder time reconstructing intercept, performance OK three predictors. Supporting visual inspection, confidence interval coverage lower smaller selection, still captures main features. intrinsic motivation interval worst, STARS interval best. Now parameters, can also helpful look mean overlap across confidence intervals, 68% case. poorer performance probably due smaller recommended sample size compared smaller selection.Finally, can look Chi-Square test assumes synthetic data model compatible observed data model. Smaller p-values suggest greater incompatibility two models. larger selection, also statistically significant, suggesting narrow utility performance ideal, inconsistent observed data.","code":"\nsynth_data2 <- syn(real_data, # return to using the original larger data set\n                   seed = my_seed) # use same seed as above ## CAUTION: Your data set has fewer observations (111) than we advise.\n## We suggest that there should be at least 170 observations\n## (100 + 10 * no. of variables used in modelling the data).\n## Please check your synthetic data carefully with functions\n## compare(), utility.tab(), and utility.gen().\n## \n## \n## Synthesis\n## -----------\n##  age gender degree_programme GSL Intrinsic STARS PASS\ncompare(synth_data2, \n        real_data, \n        stat = \"counts\")## \n## Comparing counts observed with synthetic\n## \n## Press return for next variable(s): \n## \n## Selected utility measures:\n##                      pMSE   S_pMSE df\n## age              0.003145 2.792530  2\n## gender           0.000425 0.251701  3\n## degree_programme 0.003070 1.817685  3\n## GSL              0.001869 0.829618  4\n## Intrinsic        0.000741 0.329097  4\n## STARS            0.004231 1.878619  4\n## PASS             0.002649 1.175969  4\n# Second linear model using the full three predictors\ns_lm2 <- lm.synds(PASS ~ GSL + Intrinsic + STARS,\n                 data = synth_data2)\n\n(synth_compare2 <- compare(s_lm2, # New full multiple linear regression model\n        real_data)) # Full observed data file with our 7 variables## \n## Call used to fit models to the data:\n## lm.synds(formula = PASS ~ GSL + Intrinsic + STARS, data = synth_data2)\n## \n## Differences between results based on synthetic and observed data:\n##             Synthetic   Observed       Diff Std. coef diff CI overlap\n## (Intercept) 27.316140 23.6386257  3.6775144      1.2230587  0.6879895\n## GSL         -2.308385 -1.7676992 -0.5406860     -1.1508323  0.7064149\n## Intrinsic   -1.002581 -0.1780281 -0.8245529     -1.5979280  0.5923578\n## STARS        2.280006  1.6926311  0.5873745      0.9853746  0.7486243\n## \n## Measures for one synthesis and 4 coefficients\n## Mean confidence interval overlap:  0.6838466\n## Mean absolute std. coef diff:  1.239298\n## \n## Mahalanobis distance ratio for lack-of-fit (target 1.0): 1.92\n## Lack-of-fit test: 7.683872; p-value 0.1039 for test that synthesis model is\n## compatible with a chi-squared test with 4 degrees of freedom.\n## \n## Confidence interval plot:"},{"path":"creating-synthetic-datasets.html","id":"summary","chapter":"2 Creating synthetic datasets","heading":"2.4 Summary","text":"tutorial, explored create synthetic data sets context scholarship teaching learning. often work sensitive data risk anonymity participants may prevent us accessing sharing data. Open scholarship practices recognise role value sharing research data, synthetic data sets can provide useful compromise scientific ethical responsibilities.created synthetic data sets smaller larger selection variables unpublished replication attempt Dunn (2014). assessing well synthetic data reconstructs observed data, general narrow utility measures. General utility measures include statistics like pMSE (propensity score mean-squared error), whereas narrow utility measures include comparing model parameters like regression coefficient confidence interval. saw performance worse greater mismatch recommended actual sample size, keep package recommendations mind creating synthetic data.tried provide relatively non-technical introduction synthetic data sets common world statistical agencies large granular data sets increase risk re-identification. resources, recommend primer Quintana (2020) included additional sections exploring different levels skew missing data affect synthetic data process. technical details, recommend original package article Nowok et al. (2016) synthpop website includes vignettes list resources learning synthetic data.final remember, always include label instructions informing readers provide synthetic data, mistake real observed data.","code":""},{"path":"creating-synthetic-datasets.html","id":"references","chapter":"2 Creating synthetic datasets","heading":"2.5 References","text":"Meyer, M. N. (2018). Practical Tips Ethical Data Sharing. Advances Methods Practices Psychological Science, 1(1), 131–144. https://doi.org/10.1177/2515245917747656Nowok, B., Raab, G. M., & Dibben, C. (2016). synthpop: Bespoke Creation Synthetic Data R. Journal Statistical Software, 74, 1–26. https://doi.org/10.18637/jss.v074.i11Quintana, D. S. (2020). synthetic dataset primer biobehavioural sciences promote reproducibility hypothesis generation. ELife, 9, e53275. https://doi.org/10.7554/eLife.53275Raab, G. M., Nowok, B., & Dibben, C. (2021). Assessing, visualizing improving utility synthetic data (arXiv:2109.12717). arXiv. https://doi.org/10.48550/arXiv.2109.12717Snoke, J., Raab, G. M., Nowok, B., Dibben, C., & Slavkovic, . (2018). General specific utility measures synthetic data. Journal Royal Statistical Society: Series (Statistics Society), 181(3), 663–688. https://doi.org/10.1111/rssa.12358Wickham, H. & Grolemund, G. (2017). R Data Science. O'Reilly.","code":""},{"path":"moodle-assignment-submission-reports.html","id":"moodle-assignment-submission-reports","chapter":"3 Moodle Assignment submission reports","heading":"3 Moodle Assignment submission reports","text":"Lead author: Emily NordmannPlatforms like Moodle provide extremely detailed assignment submission reports contain data student submitted, extensions, grade. tutorial explains use R create series simple reports data provided Moodle Assignment Submission reports can helpful generating insights student behavior course. tutorial designed Moodle report mind, possible adapt much code reports produced systems.","code":""},{"path":"moodle-assignment-submission-reports.html","id":"set-up","chapter":"3 Moodle Assignment submission reports","heading":"3.1 Set-up","text":"Install (required) load following packagesThe data file simulated assignment submission report 300 students essay submission. Although data simulated, file identical one downloaded Moodle able use code assignment submission reports downloaded Moodle (assuming variable names change institutions!).assignment submission report .xlsx file. lots functions can use R import specific types files, however, import function rio package great wrapper function works pretty much type file avoids remember function need specific file type.first three lines assignment submission report file blank main data table starts line 4, add skip = 3 skip first three lines data import.","code":"\nlibrary(tidyverse) # data wrangling and visualisation\nlibrary(rio) # import data\nlibrary(lubridate) # working with dates\nlibrary(plotly) # interactive plots\nlibrary(psych) # for descriptive stats\ndat <- import(\"https://github.com/emilynordmann/scholaRship/raw/master/book/data/essay_data.xlsx\", skip = 3)"},{"path":"moodle-assignment-submission-reports.html","id":"data-types","chapter":"3 Moodle Assignment submission reports","heading":"3.2 Data types","text":"anything need little work tidy file ensure R data encoded right type. First, check data using str():Based , three things need :Remove - Moodle uses represent missing data replace actual empty cell (.e., NA) using mutate() na_ifConvert date time variables date/time class R currently stored character. Modified (time student last changed file, .e., submitted), Extension, Released currently format day-month-year-hour-minute use dmy_hm function lubridate package.Convert numeric data numeric. variable character text , R read variable character. Moodle used - encode missing values, numeric Turnitin score represented character variable gotten rid - need convert back.lot can data contained file, suggestions insights can generate.","code":"\nstr(dat)## 'data.frame':    300 obs. of  14 variables:\n##  $ #             : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ Username      : chr  \"Name 1\" \"Name 2\" \"Name 3\" \"Name 4\" ...\n##  $ Participant No: chr  \"100000252\" \"100000128\" \"100000244\" \"100000587\" ...\n##  $ Email address : chr  \"myemail@student.ac.uk\" \"myemail@student.ac.uk\" \"myemail@student.ac.uk\" \"myemail@student.ac.uk\" ...\n##  $ ID number     : chr  \"1234818\" \"1234694\" \"1234810\" \"1235153\" ...\n##  $ Groups        : chr  \"group6\" \"group10\" \"group8\" \"group7\" ...\n##  $ Status        : chr  \"submitted\" \"submitted\" \"submitted\" \"submitted\" ...\n##  $ Grade         : chr  \"C3\" \"C1\" \"A4\" \"C1\" ...\n##  $ Turnitin      : chr  \"16\" \"16\" \"12\" \"15\" ...\n##  $ Grader        : chr  \"Marker 4\" \"Marker 4\" \"Marker 4\" \"Marker 3\" ...\n##  $ Modified      : chr  \"16/11/20, 20:14\" \"13/11/20, 08:26\" \"13/11/20, 08:50\" \"12/11/20, 18:05\" ...\n##  $ Released      : chr  \"7/12/20, 12:01\" \"7/12/20, 12:01\" \"7/12/20, 12:01\" \"7/12/20, 12:00\" ...\n##  $ Extension     : chr  \"16/11/20, 12:00\" \"-\" \"-\" \"-\" ...\n##  $ Files         : chr  \"essay1\" \"essay2\" \"essay3\" \"essay4\" ...\ndat_cleaned <- dat %>%\n  mutate(across(where(is.character), ~na_if(., \"-\"))) %>% # replace - with NAs\n  mutate(Modified = dmy_hm(Modified),\n         Extension = dmy_hm(Extension),\n         Released = dmy_hm(Released),\n         Turnitin = as.numeric(Turnitin))"},{"path":"moodle-assignment-submission-reports.html","id":"extensions-and-late-submissions","chapter":"3 Moodle Assignment submission reports","heading":"3.3 Extensions and late submissions","text":"Rates extensions late submission increased significantly covid found helpful look patterns students submit work relative deadline.First, create date time variable contains deadline. use format therefore use dmy_hm function convert information right format R recognises date. deadline state time assignment become late submission - us one minute stated deadline example 1 minute past 12 noon 13th November 2020.also helpful variables store many students course total, many submitted.ways can look extension late submission data. First, calculate number extensions applied simple count. code gives us number extensions day.get total number extensions, adapt code remove NAs calculation sum total calculate percent submissions extension applied:might want something fine-grained need look numbers -time submissions, extensions, late submissions, non-submissions requires bit data wrangling.case_when() allows recode values based multiple -statements. conditions :essay submitted deadline, labelled -time.essay submitted deadline extension deadline, labelled -time extension.\n3 essay submitted deadline extension applied, labelled late.essay submitted labelled non-submission.makes head explode, please know code took half hour multiple errors write got logic correct.can use new categories calculate descriptives:remove non-submissions distribution plot otherwise plotted submitted assignment first opened:can also produce interactive version plot using ggplotly() - can hover bars see counts remove certain groups data:extensions applied Moodle just wanted look difference -time submissions collapse lates extensions one group following:","code":"\ndeadline <-dmy_hm(\"13/11/2020 12:01\")\ntotal_students <- nrow(dat_cleaned)\ntotal_submissions <- dat_cleaned %>%\n  filter(Status == \"submitted\") %>%\n  nrow()\ndat_cleaned %>%\n  count(Extension)\ndat_cleaned %>%\n  count(Extension) %>%\n  filter(!is.na(Extension)) %>% # remove rows that don't have an extension date\n  summarise(total = sum(n),\n            percent = round((total/total_submissions)*100, 2))\ndat_cleaned <- dat_cleaned %>%\n  mutate(submission_category= case_when((Status == \"submitted\" & Modified <= deadline)~ \"On-time\",\n                            (Modified > deadline & deadline <= Extension) ~ \"On-time w extension\",\n                            (Modified > deadline & is.na(Extension)) ~ \"Late\",\n                            (Modified > Extension) ~ \"Late with extension\",\n                            TRUE ~ \"Non-submission\"))\ndat_cleaned %>%\n  count(submission_category) %>%\n  mutate(percent = round((n/total_submissions)*100, 2))\np1 <- dat_cleaned %>%\n  filter(submission_category != \"Non-submission\") %>%\n  ggplot(aes(Modified, fill = submission_category)) +\n  geom_histogram(colour = \"black\") +\n  theme_minimal() + # add theme\n  theme(legend.position = \"bottom\") + # move legend to bottom\n  labs(fill = NULL, x = NULL, title = \"Assignment submission report\") + #labels\n  scale_fill_viridis_d(option = \"E\") + # colour blind friendly palette\n  theme(axis.text.x = element_text(angle=90)) + # rotate axis labels\n  scale_x_datetime(date_breaks = \"1 day\", date_labels = \"%b %d\") + # set breaks\n  geom_vline(xintercept = deadline, colour = \"red\", linetype = \"dashed\", size = 1.5) # add dashed line## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\np1\nggplotly(p1)\ndat_cleaned <- dat_cleaned %>%\n  mutate(submission_category_blunt= case_when((Status == \"submitted\" & \n                                                 Modified <= deadline)~ \"On-time\",\n                            (Modified > deadline) ~ \"Late or extension\",\n                            \n                            TRUE ~ \"Non-submission\"))"},{"path":"moodle-assignment-submission-reports.html","id":"grades","chapter":"3 Moodle Assignment submission reports","heading":"3.4 Grades","text":"use 22-point scale alphanumeric grade corresponding grade point total 22-point scale (e.g., B2 = 16).grades Moodle stored alphanumeric form need convert numbers.often easiest way spreadsheet contains grades associated gradepoints, import , use inner_join() combine two files. join two files common columns, now variable Points dataset corresponding number alphanumeric grade.can now create basic descriptive stats visualisations grade point values. describe() function psych() library great quickly producing range descriptive statistics.stage want highlight simulated data read anything actual patterns, real data, patterns see meaninglessWe also look distribution grades submission category:descriptives category:associated visualization:correlation Turnitin score grade (even real data think bit pointless never know):relationship Turnitin score submission category - code removes two outliers - remove filter line put back .associated visualisation:Finally, also look grades marker:Violin-boxplots:Finally, rather using grouped histograms done previously, better visualise distributions different markers using facet_wrap() makes easier compare distributions:","code":"\ngrade_points <- import(\"https://raw.githubusercontent.com/emilynordmann/scholaRship/master/book/data/grade_points.csv\")\ndat_cleaned <- inner_join(dat_cleaned, grade_points, by = \"Grade\")\ndat_cleaned %>%\n  select(Points) %>% # just select points column for stats\n  describe()\nggplot(dat_cleaned, aes(Points)) +\n  geom_histogram(binwidth = 1, colour = \"black\") +\n  theme_minimal() +\n  labs(title = \"Distribution of essay grades\", \n       x = NULL, \n       colour = NULL, \n       subtitle = \"Dashed line = mean grade\") +\n  scale_x_continuous(breaks = seq(1,22, by = 1)) + \n  geom_vline(aes(xintercept=mean(Points),color=\"red\"), \n             linetype=\"dashed\",\n             size = 1, \n             show.legend = FALSE) \nggplot(dat_cleaned, aes(Points, fill = submission_category)) +\n  geom_histogram(binwidth = 1, colour = \"black\") +\n  theme_minimal() +\n  labs(title = \"Distribution of essay grades\", \n       x = NULL, \n       subtitle = \"Dashed line = mean grade\",\n       fill = NULL) +\n  geom_vline(aes(xintercept=mean(Points),color=\"red\"), \n             linetype=\"dashed\",\n             size=1,\n             show.legend = FALSE) +\n  scale_x_continuous(breaks = seq(1,22, by = 1)) +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d(option = \"E\")\ndat_cleaned %>%\n  group_by(submission_category) %>%\n  summarise(mean_grade = mean(Points, na.rm = TRUE),\n            median_grade = median(Points, na.rm = TRUE))\nggplot(dat_cleaned, aes(x = submission_category, y = Points, fill = submission_category)) +\n  geom_violin(show.legend = FALSE,\n              alpha = .4) +\n  geom_boxplot(width = .2, show.legend = FALSE) +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\") +\n  labs(x = NULL, title = \"Grade point by submission category\")\nggplot(dat_cleaned, aes(Points, Turnitin)) +\n  geom_jitter() + # use jitter rather than geom_point as some overlapping data points\n  geom_smooth(method = \"loess\") + # no clear linear relationship, otherwise use method = \"lm\"\n  labs(x = \"Grade point\", y = \"Turnitin score\")## `geom_smooth()` using formula = 'y ~ x'\ndat_cleaned %>%\n  filter(Turnitin < 75) %>%\n  group_by(submission_category) %>%\n  summarise(mean_grade = mean(Turnitin, na.rm = TRUE),\n            median_grade = median(Turnitin, na.rm = TRUE))\ndat_cleaned %>%\n  filter(Turnitin < 75) %>%\n  ggplot(aes(x = submission_category, y = Turnitin, fill = submission_category)) +\n  geom_violin(show.legend = FALSE,\n              alpha = .4) +\n  geom_boxplot(width = .2, show.legend = FALSE) +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\") +\n  labs(x = NULL, title = \"Turnitin score by submission category\")\ndat_cleaned %>%\n  group_by(Grader) %>%\n  summarise(mean_grade = mean(Points, na.rm = TRUE),\n            median_grade = median(Points, na.rm = TRUE))\ndat_cleaned %>%\n  ggplot(aes(x = Grader, y = Points, fill = Grader)) +\n  geom_violin(show.legend = FALSE,\n              alpha = .4) +\n  geom_boxplot(width = .2, show.legend = FALSE) +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\") +\n  labs(x = NULL, title = \"Grade point by marker\")\nggplot(dat_cleaned, aes(Points, fill = Grader)) +\n  geom_histogram(binwidth = 1, colour = \"black\", show.legend = FALSE) +\n  theme_minimal() +\n  labs(title = \"Distribution of essay grades\", \n       x = \"Grade point\", \n       subtitle = \"Dashed line = mean grade\",\n       fill = NULL) +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d(option = \"E\") +\n  facet_wrap(~Grader, nrow = 4)"},{"path":"anonymous.html","id":"anonymous","chapter":"4 Creating Anonymous Data From Synthetic Data","heading":"4 Creating Anonymous Data From Synthetic Data","text":"developed following chapters specifically Association Learning Technology (Alt) 2023 conference.","code":""},{"path":"workshop.html","id":"workshop","chapter":"5 ALT Conference Prep","heading":"5 ALT Conference Prep","text":"attending workshop ALT conference like live-code along us workshop, please complete set-chapter advance everything need.","code":""},{"path":"workshop.html","id":"installing-r-and-rstudio","chapter":"5 ALT Conference Prep","heading":"5.1 Installing R and RStudio","text":"R RStudio already installed, please follow instructions appendix come back chapter.","code":""},{"path":"workshop.html","id":"workshop-prep","chapter":"5 ALT Conference Prep","heading":"5.2 Workshop prep","text":"worked R basic familiarity code R objects functions work, install packages tidyverse,plotly, scales, ggpubr skip set-check end chapter.R novice, find helpful work workshop.","code":""},{"path":"workshop.html","id":"intro-r-rstudio","chapter":"5 ALT Conference Prep","heading":"5.3 R and RStudio","text":"R programming language write code RStudio Integrated Development Environment (IDE) makes working R easier. Think knowing English using plain text editor like NotePad write book versus using word processor like Microsoft Word. , much harder without things like spell-checking formatting able use advanced features Word developed. similar way, can use R without R Studio recommend . RStudio serves text editor, file manager, spreadsheet viewer, . key thing remember although work using RStudio workshop, actually using two pieces software means time--time, may separate updates.","code":""},{"path":"workshop.html","id":"rstudio_ide","chapter":"5 ALT Conference Prep","heading":"5.3.1 RStudio","text":"installed R, gave computer ability process R programming language, also installed app called \"R\". never use app. Instead, use RStudio. RStudio arranged four window panes.\nFigure 5.1: RStudio IDE\ndefault, upper left pane source pane, view, write, edit code files view data tables spreadsheet format. first open RStudio, pane display open document load data - worry, get soon.lower left pane console pane, can type commands view output messages. can write code console test . code run can create objects environment, code saved. need write code script source pane save .right panes several different tabs show information code. used tabs upper right pane Environment tab Help tab. environment tab lists information objects defined code. learn Help tab Section ??.lower right pane, used tabs Files tab directory structure, Plots tab plots made script, Packages tab managing add-packages (see Section 5.6), Viewer tab display reports created scripts. can change location panes tabs shown Preferences > Pane Layout.","code":""},{"path":"workshop.html","id":"intro-reproducibility","chapter":"5 ALT Conference Prep","heading":"5.3.2 Reproducibility","text":"One main reasons learn R can create reproducible reports. involves writing scripts transform data, create summaries visualisations, embed report way always gives results.things reproducibly, others (future ) can understand check work. can also reuse work easily. example, need create exam board report every semester student grades, reproducible report allows download new data create report within seconds. might take little longer set report first instance reproducible methods, time saves long run invaluable.Section .4 shows change two important settings global Options increase reproducibility. settings :Restore .RData workspace startup: CheckedNot CheckedSave workspace .RData exit: AlwaysNeverAsk","code":""},{"path":"workshop.html","id":"themes-and-accessiblilty","chapter":"5 ALT Conference Prep","heading":"5.3.3 Themes and accessiblilty","text":"can customise R Studio looks make work . Click Tools - Global Options - Appearance. can change default font, font size, general appearance R Studio, including using dark mode.","code":""},{"path":"workshop.html","id":"intro-sessions","chapter":"5 ALT Conference Prep","heading":"5.4 Sessions","text":"settings configured correctly, open RStudio start writing code, loading packages, creating objects, new session Environment tab completely empty. find code working figure , might worth restarting R session. clear environment detach loaded packages - think like restarting phone. several ways can restart R:Menu: Session > Restart RCmd-Shift-F10 Ctl-Shift-F10type .rs.restartR() console","code":""},{"path":"workshop.html","id":"functions","chapter":"5 ALT Conference Prep","heading":"5.5 Functions","text":"install R access range functions including options data wrangling statistical analysis. functions included default installation typically referred base R can think like default apps come pre-loaded phone.function name refers code can reuse. using functions provided packages, can also write functions.type function console pane, run soon hit enter. put function script R Markdown document source pane, run run script, knit R Markdown file, run code chunk. learn workshop.example, function sum() included base R, expect. console, run code:","code":"\nsum(1,2,3)## [1] 6"},{"path":"workshop.html","id":"packages","chapter":"5 ALT Conference Prep","heading":"5.6 Packages","text":"One great things R, however, user extensible: anyone can create new add-extends functionality. currently thousands packages R users created solve many different kinds problems, just simply fun. example, packages data visualisation, machine learning, interactive dashboards, web scraping, playing games Sudoku.Add-packages distributed base R, downloaded installed archive, way , instance, download install PokemonGo smartphone. main repository packages reside called CRAN, Comprehensive R Archive Network.important distinction installing package loading package.","code":""},{"path":"workshop.html","id":"install-package","chapter":"5 ALT Conference Prep","heading":"5.6.1 Installing a package","text":"done using install.packages(). like installing app phone: app remain installed remove . instance, want use PokemonGo phone, install App Store Play Store; re-install time want use . launch app, run background close restart phone. Likewise, install package, package available (loaded) every time open R.Install tidyverse package system. package main package use throughout book data wrangling, summaries, visualisation.get message says something like package ‘tidyverse’ successfully unpacked MD5 sums checked, installation successful. get error package installed, check troubleshooting section Appendix .10.Never install package inside script. console pane.can also install multiple packages . command install packages using workshop.","code":"\n# type this in the console pane\ninstall.packages(\"tidyverse\")\npackages <- c(\n  \"tidyverse\",  # for everything\n  \"plotly\",  # Creates interactive plots\n  \"ggpubr\",   # Builds on ggplot2 to build specific publication ready plots\n  \"scales\" # for plot scales\n)\n\ninstall.packages(packages)"},{"path":"workshop.html","id":"loading-a-package","chapter":"5 ALT Conference Prep","heading":"5.6.2 Loading a package","text":"done using library() function. like launching app phone: functionality app launched remains close app restart. example, run library(patchwork) within session, functions package referred plotly made available R session. next time start R, need run library(plotly) want access package.installing thetidyverse package, can load current R session follows:might get red text load package, normal. usually warning package functions name packages already loaded.can use convention package::function() indicate add-package function resides. instance, see readr::read_csv(), refers function read_csv() readr add-package. package loaded using library(), specify package name function unless conflict (e.g., two packages loaded function name).","code":"\nlibrary(tidyverse)"},{"path":"workshop.html","id":"tidyverse","chapter":"5 ALT Conference Prep","heading":"5.6.3 Tidyverse","text":"tidyverseis meta-package loads several packages incredibly useful cleaning, processing, summarising, visualising almost type data:ggplot2, data visualisationreadr, data importtibble, tablestidyr, data tidyingdplyr, data manipulationlubridate dates timesstringr, stringsforcats, factorspurrr, repeating things","code":""},{"path":"workshop.html","id":"using-functions","chapter":"5 ALT Conference Prep","heading":"5.7 Using functions","text":"","code":""},{"path":"workshop.html","id":"arguments","chapter":"5 ALT Conference Prep","heading":"5.7.1 Arguments","text":"functions allow/require specify one morearguments. options can set. can look arguments/options function using help documentation. arguments required, optional. Optional arguments often use default (normally specified help documentation) enter value.example, look help documentation function sample() randomly samples items list.help documentation sample() appear bottom right help panel. usage section, see sample() takes following form:arguments section, explanations arguments. x list items want choose , size number items want choose, replace whether item may selected , prob gives probability item chosen. details section notes values entered replace prob use defaults FALSE (item can chosen ) NULL (items equal probability chosen). default value x size, must specified otherwise code run.try example just change required arguments x size ask R choose 5 random letters (letters built-vector 26 lower-case Latin letters).sample() generates random sample. time run code, generate different set random letters (try ). function set.seed() controls random number generator - using functions use randomness (sample()), running set.seed() ensure get result (many cases may want ). get numbers , run set.seed(1242016) console, run sample(x = letters, size = 5) .Now can change default value replace argument produce set letters allowed duplicates.time R still produced 5 random letters, now set letters two instances \"k\". Always remember use help documentation help understand arguments function requires.","code":"\n?sample\nsample(x, size, replace = FALSE, prob = NULL)\nsample(x = letters, size = 5)## [1] \"z\" \"v\" \"y\" \"w\" \"j\"\nset.seed(8675309)\nsample(x = letters, size = 5, replace = TRUE)## [1] \"t\" \"k\" \"j\" \"k\" \"m\""},{"path":"workshop.html","id":"argument-names","chapter":"5 ALT Conference Prep","heading":"5.7.2 Argument names","text":"examples, written argument names code (.e., x, size, replace), however, strictly necessary. following two lines code produce result (although time run sample() produce slightly different result, random, still work ):Importantly, write argument names, R use default order arguments. sample assume first value enter x. second value size third value replace.write argument names can write arguments whatever order like:first learning R, may find useful write argument names can help remember understand part function . However, skills progress may find quicker omit argument names also see examples code online use argument names, important able understand argument bit code referring (look help documentation check).workshop, always write argument names first time use function. However, subsequent uses may omitted.","code":"\nsample(x = letters, size = 5, replace = TRUE)\nsample(letters, 5, TRUE)\nsample(size = 5, replace = TRUE, x = letters)"},{"path":"workshop.html","id":"tab-auto-complete","chapter":"5 ALT Conference Prep","heading":"5.7.3 Tab auto-complete","text":"One useful feature R Studio tab auto-complete functions (see Figure 5.2). write name function press tab key, R Studio show arguments function takes along brief description. press enter argument name fill name , just like auto-complete phone. incredibly useful first learning R remember use feature frequently.\nFigure 5.2: Tab auto-complete\n","code":""},{"path":"workshop.html","id":"objects","chapter":"5 ALT Conference Prep","heading":"5.8 Objects","text":"large part coding involve creating manipulating objects. Objects contain stuff. stuff can numbers, words, result operations analyses. assign content object using <-.Run following code console, change values name age details change christmas holiday date care .see four objects now appear environment pane:name character (text) data. order R recognise character data, must enclosed double quotation marks \" \".age numeric data. order R recognise number, must enclosed quotation marks.today stores result function Sys.Date(). function returns computer system's date. Unlike name age, hard-coded (.e., always return values enter), contents object today change dynamically date. , run function tomorrow, update date tomorrow's date.christmas also date hard-coded specific date. wrapped within .Date() function tells R interpret character string provide date rather text.print contents object, type object's name console press enter. Try printing four objects now.Finally, key concept understand objects can interact can save results interactions new object. Edit run following code create new objects, print contents new object.","code":"\nname <- \"Emily\"\nage <- 38\ntoday <- Sys.Date()\nchristmas <- as.Date(\"2023-12-25\")\ndecade <- age + 10\nfull_name <- paste(name, \"Nordmann\")\nhow_long <- christmas - today"},{"path":"workshop.html","id":"help","chapter":"5 ALT Conference Prep","heading":"5.9 Getting help","text":"feel like need lot help starting learn. really go away; impossible memorise everything. goal learn enough structure R can look things quickly. introduce specialised jargon glossary; easier google \"convert character numeric R\" \"make numbers quotes actual numbers words\". addition function help described , additional resources use often.","code":""},{"path":"workshop.html","id":"package-reference-manuals","chapter":"5 ALT Conference Prep","heading":"5.9.1 Package reference manuals","text":"Start help browser entering help.start() console. Click \"Packages\" \"Reference\" see list packages. Scroll readxl package click see list functions available package.","code":""},{"path":"workshop.html","id":"googling","chapter":"5 ALT Conference Prep","heading":"5.9.2 Googling","text":"function help help, even sure function need, try Googling question. take practice able use right jargon search terms get want. helps put \"R\" \"tidyverse\" search text, name relevant package, like ggplot2.","code":""},{"path":"workshop.html","id":"vignettes","chapter":"5 ALT Conference Prep","heading":"5.9.3 Vignettes","text":"Many packages, especially tidyverse ones, helpful websites vignettes explaining use functions. vignettes also available inside R. can access package's help page vignette() function.","code":"\n# opens a list of available vignettes\nvignette(package = \"ggplot2\")\n\n# opens a specific vignette in the Help pane\nvignette(\"ggplot2-specs\", package = \"ggplot2\")"},{"path":"workshop.html","id":"setup-check","chapter":"5 ALT Conference Prep","heading":"5.10 Workshop set-up check","text":"Restart R session run code copying pasting console hitting enter. managed install update software packages required, run without issue produce histograms. produce messages look like errors involving stat_bin non-finite values, worry, errors explain messages mean workshop.get error package called..., make sure installed packages listed Section 5.6.1.technical issues working machine get code run, please use RStudio Cloud workshop time troubleshoot installation problems.","code":"\nlibrary(tidyverse)\nlibrary(ggpubr)\n\n# Using ggplot2 to visualize distribution of height\ngg_height <- ggplot(starwars, aes(x = height)) + \n  geom_histogram(binwidth = 10, fill = \"blue\", alpha = 0.7) + \n  labs(title = \"Distribution of Heights of Star Wars Characters\", x = \"Height (cm)\", y = \"Count\")\n\n# Using ggplot2 to visualize distribution of mass\ngg_mass <- ggplot(starwars, aes(x = mass)) + \n  geom_histogram(binwidth = 10, fill = \"red\", alpha = 0.7) + \n  labs(title = \"Distribution of Mass of Star Wars Characters\", x = \"Mass (kg)\", y = \"Count\")\n\n# Displaying the plots side-by-side using ggpubr\nggarrange(gg_height, gg_mass, ncol = 2, common.legend = TRUE)"},{"path":"workshop.html","id":"glossary-intro","chapter":"5 ALT Conference Prep","heading":"5.11 Glossary","text":"","code":""},{"path":"workshop.html","id":"resources-intro","chapter":"5 ALT Conference Prep","heading":"5.12 Further Resources","text":"RStudio IDE CheatsheetRStudio Cloud","code":""},{"path":"Echo_tidy.html","id":"Echo_tidy","chapter":"6 Getting started with Echo360 data in R with tidyverse","heading":"6 Getting started with Echo360 data in R with tidyverse","text":"","code":""},{"path":"Echo_tidy.html","id":"reading-downloaded-data-into-r","chapter":"6 Getting started with Echo360 data in R with tidyverse","heading":"6.1 Reading downloaded data into R","text":"start tutorial, install following packages. note comment need package .readr package tidyverse provides us host functions reading data R. Often course, multiple video recordings Echo360 data . useful keep data within one data frame R analysis, often consider metrics across full course one video.list.files command list files available within given working directory. download data tutorial, please download .zip file containing 9 files Echo360 data. working directory, folder called \"data\", created subfolder called \"Echo360_Data\" place files. See Chapter 4 created anonymous synthetic data use tutorials. data stored .csv files can specifically list files using command pattern = \".csv\" shown :object created (file_name) contains names various Echo360 videos, though naming conventions can times quite awkward handle R. can simply number videos using code .","code":"\nlibrary(tidyverse) # Package of packages for plotting and wrangling \nlibrary(plotly) # Creates interactive plots \nlibrary(ggpubr) # Builds on ggplot2 to build specific publication ready plots \n# Obtain list of files from directory\nfiles <- list.files(path = \"data/Echo360_Data/\", \n                    pattern=\".csv\") \n\n# Read in all files\ndata <- read_csv(paste0(\"data/Echo360_Data/\", files), # Add our working directory to the list of files \n                 id=\"video\") # What should be call the column containing the name of the video file? \n\n# Fill in spaces between column names\ndata <- tibble(data, \n               .name_repair = \"universal\") # Setting to universal makes all names unique and syntactic\n# To break this code down, we start with the innermost function\n# 1. We first make each unique video name a factor (unique category)\n# 2. We then make each factor a number, so we get an ascending number from 1 to 9\ndata$video <- as.numeric(as.factor(data$video))"},{"path":"Echo_tidy.html","id":"data-descriptions-for-each-field-in-downloaded-data","chapter":"6 Getting started with Echo360 data in R with tidyverse","heading":"6.2 Data descriptions for each field in downloaded data","text":"Within Echo360 data, various fields data different formats. important understand fields corresponds exploratory analysis data.R often read data correct format store variables right way. However, always case, best check carried correctly. can obtain summary column data using str command:output command details variable type column stored R. example, owner_name character variable, whereas total_views numerical.point, worthwhile checking raw data (visually inspecting .csv file(s) read ) output str() check R correctly converted column preferred variable type.","code":"\nstr(data)## tibble [1,466 × 16] (S3: tbl_df/tbl/data.frame)\n##  $ video            : num [1:1466] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ media_id         : chr [1:1466] \"631c9eb6-7828-4e41-9e9e-ae3b261ae741\" \"631c9eb6-7828-4e41-9e9e-ae3b261ae741\" \"631c9eb6-7828-4e41-9e9e-ae3b261ae741\" \"631c9eb6-7828-4e41-9e9e-ae3b261ae741\" ...\n##  $ media_name       : chr [1:1466] \"Physiological Psychology Week 1 Part 1\" \"Physiological Psychology Week 1 Part 1\" \"Physiological Psychology Week 1 Part 1\" \"Physiological Psychology Week 1 Part 1\" ...\n##  $ create_date      : chr [1:1466] \"01/07/2022\" \"01/07/2022\" \"01/07/2022\" \"01/07/2022\" ...\n##  $ duration         : 'hms' num [1:1466] 00:13:11 00:13:11 00:13:11 00:13:11 ...\n##   ..- attr(*, \"units\")= chr \"secs\"\n##  $ owner_name       : chr [1:1466] \"James Bartlett\" \"James Bartlett\" \"James Bartlett\" \"James Bartlett\" ...\n##  $ course           : chr [1:1466] \"Physiological Psychology (PSYCH4065/5029) 2022-23\" \"Physiological Psychology (PSYCH4065/5029) 2022-23\" \"Physiological Psychology (PSYCH4065/5029) 2022-23\" \"Physiological Psychology (PSYCH4065/5029) 2022-23\" ...\n##  $ user_name        : chr [1:1466] \"Ianto, Tasadduq\" \"Alessa, Tamarah\" \"Mary-Ann, Abdussalam\" \"Aderyn, Cahil\" ...\n##  $ email_address    : chr [1:1466] \"210396@university.ac.uk\" \"205650@university.ac.uk\" \"211437@university.ac.uk\" \"298179@university.ac.uk\" ...\n##  $ total_views      : num [1:1466] 1 1 1 2 2 4 2 2 1 1 ...\n##  $ total_view_time  : 'hms' num [1:1466] 00:10:55 00:12:46 00:13:09 00:10:32 ...\n##   ..- attr(*, \"units\")= chr \"secs\"\n##  $ average_view_time: 'hms' num [1:1466] 00:10:55 00:12:46 00:13:09 00:05:16 ...\n##   ..- attr(*, \"units\")= chr \"secs\"\n##  $ on_demand_views  : num [1:1466] 1 1 1 2 2 4 2 2 1 1 ...\n##  $ live_view_count  : num [1:1466] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ downloads        : num [1:1466] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ last_viewed      : chr [1:1466] \"01/16/2023\" \"01/18/2023\" \"01/24/2023\" \"01/15/2023\" ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   media_id = col_character(),\n##   ..   media_name = col_character(),\n##   ..   create_date = col_character(),\n##   ..   duration = col_time(format = \"\"),\n##   ..   owner_name = col_character(),\n##   ..   course = col_character(),\n##   ..   user_name = col_character(),\n##   ..   email_address = col_character(),\n##   ..   total_views = col_double(),\n##   ..   total_view_time = col_time(format = \"\"),\n##   ..   average_view_time = col_time(format = \"\"),\n##   ..   on_demand_views = col_double(),\n##   ..   live_view_count = col_double(),\n##   ..   downloads = col_double(),\n##   ..   last_viewed = col_character()\n##   .. )\n##  - attr(*, \"problems\")=<externalptr>\n# From James - str or glimpse work better? \nglimpse(data)## Rows: 1,466\n## Columns: 16\n## $ video             <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ media_id          <chr> \"631c9eb6-7828-4e41-9e9e-ae3b261ae741\", \"631c9eb6-78…\n## $ media_name        <chr> \"Physiological Psychology Week 1 Part 1\", \"Physiolog…\n## $ create_date       <chr> \"01/07/2022\", \"01/07/2022\", \"01/07/2022\", \"01/07/202…\n## $ duration          <time> 00:13:11, 00:13:11, 00:13:11, 00:13:11, 00:13:11, 0…\n## $ owner_name        <chr> \"James Bartlett\", \"James Bartlett\", \"James Bartlett\"…\n## $ course            <chr> \"Physiological Psychology (PSYCH4065/5029) 2022-23\",…\n## $ user_name         <chr> \"Ianto, Tasadduq\", \"Alessa, Tamarah\", \"Mary-Ann, Abd…\n## $ email_address     <chr> \"210396@university.ac.uk\", \"205650@university.ac.uk\"…\n## $ total_views       <dbl> 1, 1, 1, 2, 2, 4, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2…\n## $ total_view_time   <time> 00:10:55, 00:12:46, 00:13:09, 00:10:32, 00:19:58, 0…\n## $ average_view_time <time> 00:10:55, 00:12:46, 00:13:09, 00:05:16, 00:09:59, 0…\n## $ on_demand_views   <dbl> 1, 1, 1, 2, 2, 4, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2…\n## $ live_view_count   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ downloads         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ last_viewed       <chr> \"01/16/2023\", \"01/18/2023\", \"01/24/2023\", \"01/15/202…"},{"path":"Echo_tidy.html","id":"quick-summaries-of-data","chapter":"6 Getting started with Echo360 data in R with tidyverse","heading":"6.3 Quick summaries of data","text":"can obtain quick summaries variables get early feel data check potential errors outliers data.","code":""},{"path":"Echo_tidy.html","id":"numerical-data","chapter":"6 Getting started with Echo360 data in R with tidyverse","heading":"6.3.1 Numerical data","text":"numerical data, can obtain series numerical summaries. can compute mean observations numerical columns using following code:select() command allows us select variables based specific criteria (variable name condition). , select variables numeric using (.numeric). can obtain mean numeric variables using summarise_all() command specifying chosen summary metric (, mean).observations counts, can look total number observations count:group_by() function allows us carry computations groups. can use summarise() obtain total number counts total number views using n().Using group_by() summarise() can useful want add number observations addition variables, simply want count number observations, can use count() function:function scales nicely want look number observations combinations variables, example, wanted know number total_views video.","code":"\ndata %>% # The data frame you are using\n  # select chooses or omits the columns you want included\n  select(where(is.numeric)) %>% # By using where() within select(), we can ask R to show us all the numeric variables\n  summarise_all(mean) # For all the numeric variables, calculate the mean value\ndata %>% # The data frame you are using\n  group_by(total_views) %>% # Group by the total_views variable\n  summarise(n = n()) # Calculate the number of observations for each level of group_by\ndata %>% # The data frame you are using\n  count(total_views) # For each variable, count the number of observations\ndata %>% # The data frame you are using\n  count(video, total_views) %>% # For each variable, count the number of observations\n  head() # Limit the number of observations to the first 6"},{"path":"Echo_course.html","id":"Echo_course","chapter":"7 Exploring Echo360 video/course level data in R with tidyverse","heading":"7 Exploring Echo360 video/course level data in R with tidyverse","text":"","code":"\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(ggpubr)\nlibrary(scales)\n# Obtain list of files from directory\nfiles <- list.files(path = \"data/Echo360_Data/\", \n                    pattern=\".csv\") \n\n# Read in all files\ndata <- read_csv(paste0(\"data/Echo360_Data/\", files), id=\"file_name\")## Rows: 1466 Columns: 16\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (8): media_id, media_name, create_date, owner_name, course, user_name, ...\n## dbl  (4): total_views, on_demand_views, live_view_count, downloads\n## time (3): duration, total_view_time, average_view_time\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# Fill in spaces between column names\ndata <- tibble(data, .name_repair = \"universal\")\n\ndata$file_name <- as.numeric(as.factor(data$file_name))\ncolnames(data)[1] <- \"Video\""},{"path":"Echo_course.html","id":"handling-datetime-data-with-lubridate","chapter":"7 Exploring Echo360 video/course level data in R with tidyverse","heading":"7.1 Handling date/time data with lubridate","text":"Handling data date/time format R somewhat different variable types treated differently handled care. lubridate package R (loaded part tidyverse) allows us easily handle tricky date/time data extract useful variables .","code":""},{"path":"Echo_course.html","id":"converting-data-to-datetime-format","chapter":"7 Exploring Echo360 video/course level data in R with tidyverse","heading":"7.1.1 Converting data to date/time format","text":"first task convert date/time data correct format. sample data working , five variables contain data : Create.Date, Duration, Total.View.Time,Average.View.Time Last.Viewed.variables fall two types closer inspection:Create.Data & Last.Viewed - listed date (particular day)Duration, Total.View.Time Average.View.Time - listed time, recorded hours, minutes seconds.Several functions can used take string data convert desired date/time format. useful cheat sheet lubridate library contains examples functions can found . data, use mdy hms functions follows:, mutate_at() function allows us update selected variables modify specified function follows. hms converts string date/time object set hours-minutes-seconds. mdy converts string date/time object set month-day-year (note Echo360 data saves date/times US format).can take look transformations closely:can see variables converted using hmsare printed format 10M 20S. prefer visualise standard format, can use hms::as_hms() .","code":"\ndata<-data %>%\n  mutate_at(c('duration', 'total_view_time', 'average_view_time'), hms) %>%\n  mutate_at(c('create_date', 'last_viewed'), mdy)\nhead(data$duration)\nhead(data$last_viewed)## [1] \"13M 11S\" \"13M 11S\" \"13M 11S\" \"13M 11S\" \"13M 11S\" \"13M 11S\"\n## [1] \"2023-01-16\" \"2023-01-18\" \"2023-01-24\" \"2023-01-15\" \"2023-01-16\"\n## [6] \"2023-02-15\""},{"path":"Echo_course.html","id":"extracting-elements-from-datetime-data","chapter":"7 Exploring Echo360 video/course level data in R with tidyverse","heading":"7.1.2 Extracting elements from date/time data","text":"Sometimes may wish obtain specific parts date/time month, minutes etc. several functions lubridate allow us extract easily date/time object.Say wish obtain minutes Average.View.Time, can obtain easily using minute function. commands show spread duration minutes boxplotWe can also produce interactive version boxplot using plotlySuppose want look last month students viewed videos across course. can obtain applying month function Last.Viewed variableWe can easily display plot plotly using command ggplotly stored ggplot output:","code":"\ndata <- data %>% mutate(average_view_minute=minute(data$average_view_time))\n\nggplot(data,aes(y=average_view_minute)) + geom_boxplot() + labs(title=\"Boxplot of average video viewing times\") + ylab(\"View time (mins)\") + theme_bw()\nfig_avg_views <- plot_ly(data,y=~ average_view_minute,type=\"box\",name=\"\")\nfig_avg_views <- fig_avg_views %>% layout(title=\"Boxplot of average video viewing times\")\nfig_avg_views\ndata <- data %>% mutate(month_last_viewed=month(data$last_viewed))\n\nlast_viewed <- ggplot(data,aes(x=month_last_viewed)) + geom_bar() + labs(title=\"Las month videos viewed\") + ylab(\"Count\") + theme_bw() + scale_x_continuous(breaks= pretty_breaks())\nlast_viewed\nggplotly(last_viewed)"},{"path":"Echo_course.html","id":"maths-with-date-times","chapter":"7 Exploring Echo360 video/course level data in R with tidyverse","heading":"7.1.3 Maths with date-times","text":"Sometimes, may wish compare difference time certain events. , lubridate provides useful functions help us . example, can look difference average view time total view time certain video using simple arithmetic operatorsAs data returns values minutes seconds, transform one unit type visualise","code":"\ndata_video1 <- data[data$Video==1,]\ndata_video1 <- data_video1 %>% mutate(time_difference=average_view_time-duration)\nhead(data_video1$time_difference)## [1] \"-3M 44S\"  \"-1M 35S\"  \"-2S\"      \"-8M 5S\"   \"-4M 48S\"  \"-11M -9S\"\ndata_video1$time_difference <- second(data_video1$time_difference)\n\ntime_difference <- ggplot(data_video1,aes(y=time_difference)) + geom_boxplot() + labs(title=\"Boxplot of average video viewing times compared to total times\") + ylab(\"View time (seconds)\") + theme_bw() + geom_hline(yintercept=0,color=\"red\")\ntime_difference\nggplotly(time_difference)"},{"path":"Echo_course.html","id":"total-views-for-each-video","chapter":"7 Exploring Echo360 video/course level data in R with tidyverse","heading":"7.2 Total views for each video","text":"following sections mostly creating plots data summarised video level. Given data currently stored student level (one row per student per video) transform data new data set called video_data group data video create variables interest. Firstly, create new data set two columns:Video: video number stored factor (make easier plot later );Total_Views: total number views per video.can now easily create bar chart total number views per video follows:plot video number along x-axis total number views video y-axis. numbers include duplications students watched videos multiple times. may also interested total number unique views, .e. total number students watched video least . , need create new variable called unique_views contain sum rows video. can add video-level data set video_data.can edit code previous bar cahrt create bar chart total number unique student views per video follows:","code":"\nvideo_data <- data %>% group_by(Video) %>%\n  summarise(total_views=sum(total_views)) %>%\n  mutate_at('Video', factor)\nfig.total.views <- ggplot(video_data, aes(x=Video, y=total_views)) +\n  geom_bar(stat = \"identity\") + \n  labs(title=\"Total number of views per video\") + \n  ylab(\"Total views\") +\n  xlab(\"Video\") +\n  theme_bw()\n\nfig.total.views\nvideo_data <- data %>% group_by(Video) %>%\n  summarise(Total_Views=sum(total_views), unique_views=n()) %>%\n  mutate_at('Video', factor)\nfig.unique.views<-ggplot(video_data, aes(x=Video, y=unique_views)) +\n  geom_bar(stat = \"identity\") + \n  labs(title=\"Total number of unique student views per video\") + \n  ylab(\"Unique student views\") +\n  xlab(\"Video\") +\n  theme_bw()\n\nfig.unique.views"},{"path":"Echo_course.html","id":"creating-an-interactive-version-using-plotly.","chapter":"7 Exploring Echo360 video/course level data in R with tidyverse","heading":"7.2.1 Creating an interactive version using plotly.","text":"can create mor interactive plots, allowing us share information data interactive environment using plotly. First start installing loading plotly library.start creating plotly version barchart unique student views using following ggplotly() function.Advanced\ncan add additional information total views, duration video average view time hover text follows.might also want create stacked barchart total number views split unique student views repeated views. can plotly follows.","code":"\nlibrary(plotly)\nggplotly(fig.unique.views)\nvideo_data <- data %>%\n  group_by(Video) %>%\n  summarise(total_views=sum(total_views), video_data=n(), average_view_time=mean(period_to_seconds(average_view_time), na.rm=T)) %>%\n  mutate(average_view_time=round(seconds_to_period(average_view_time),0),\n         repeated_views=total_views-video_data,\n         duration = hms(c('00:10:48', '00:13:11', '00:13:55', '00:14:11', '00:14:42', '00:15:08', '00:16:20', '00:16:34', '00:18:28')),\n         percentage_viewed=average_view_time/duration*100)\n\nfig.all.views <- plot_ly(video_data,\n  x = ~as.factor(Video),\n  y = ~video_data,\n  name = \"Unique Views\",\n  type = \"bar\",\n  hovertemplate = paste0(\"Video %{x}\",\n                         \"<br>Total views:\", video_data$total_views,\n                         \"<br>Unique views: %{y}\",\n                         \"<br> Duration: \", video_data$duration,\n                         \"<br> Average view time: \", video_data$average_view_time,\n                         \"<extra><\/extra>\")\n\n)%>%\n  layout(title = 'Video views', xaxis = list(title = 'Video number'),\n         yaxis = list(title = 'Unique views'))\n\nfig.all.views\nfig.all.views <- fig.all.views %>% add_trace(y = ~repeated_views, name = 'Repeated views')\n\nfig.all.views <- fig.all.views %>% layout(yaxis = list(title = 'Count'), barmode = 'stack')\n\nfig.all.views"},{"path":"Echo_course.html","id":"video-duration-vs.-average-percentage-viewed","chapter":"7 Exploring Echo360 video/course level data in R with tidyverse","heading":"7.3 Video duration vs. average percentage viewed","text":"now create scatterplot video duration vs. average percentage video viewed using ggplot follows.Advanced\ncan add additional information video number average view time hover text follows.","code":"\nfig.duration.viewed<-ggplot(video_data, aes(x=as.numeric(duration, 'minutes'), y=percentage_viewed)) +\n  geom_point()+ \n  labs(title=\"Video duration vs. average percentage viewed\") + \n  ylab(\"Average percentage of video viewed\") +\n  xlab(\"Video duration\") +\n  theme_bw()\n\nfig.duration.viewed\nggplotly(fig.duration.viewed)\nfig.duration.viewed.2 <- plot_ly(video_data,\n               x = ~as.numeric(duration, 'minutes'),\n               y = ~percentage_viewed,\n               type = \"scatter\",\n               mode = \"markers\",\n               hovertemplate = paste0(\"Video \", video_data$Video,\n                                      \"<br>Average percentage viewed: \", round(video_data$percentage_viewed,1), \"%\",\n                                      \"<br> Duration: \", video_data$duration,\n                                      \"<br> Average view time: \", video_data$average_view_time,\n                                      \"<extra><\/extra>\"))%>%\n  layout(xaxis = list(title = 'Duration of video'),\n         yaxis = list(title = 'Percentage of video viewed'))\n\nfig.duration.viewed.2"},{"path":"Echo_combine.html","id":"Echo_combine","chapter":"8 Combining Echo360 data with other sources of data in R with tidyverse","heading":"8 Combining Echo360 data with other sources of data in R with tidyverse","text":"dplyr package tidyverse number functions merge \"join\" data. reference page states:Mutating joins add columns y x, matching observations based keys. four mutating joins: inner join, three outer joins.Inner join\ninner_join() keeps observations x matching key y.important property inner join unmatched rows either input included result. means generally inner joins appropriate analyses, easy lose observations.Outer joins\nthree outer joins keep observations appear least one data frames:left_join() keeps observations x.right_join() keeps observations y.full_join() keeps observations x y.illustrate functions consider data Echo360 together data checklist records percentage tasks student marked complete also data containing results multiple choice quizzes (MCQ). checklist MCQ data objects student appears .","code":"\nlibrary(tidyverse)\nlibrary(plotly)## \n## Attaching package: 'plotly'## The following object is masked from 'package:ggplot2':\n## \n##     last_plot## The following object is masked from 'package:stats':\n## \n##     filter## The following object is masked from 'package:graphics':\n## \n##     layout\nlibrary(ggpubr)"},{"path":"Echo_combine.html","id":"merging-data-where-each-student-only-appears-once-in-each-data-object","chapter":"8 Combining Echo360 data with other sources of data in R with tidyverse","heading":"8.1 Merging data where each student only appears once in each data object","text":"interested combining Echo360 data just one lecture (thus student appear ) together MCQ data follow following steps.First select variables interest data object, Lectures_01 contains Echo360 data first lecture.Second, specify type 'join', e.g.Third, check input ouput data objects expect. example dim() names() functions return dimensions variable names object.note following properties, expected:type join/merge returns variablesthe left join retains 67 student email addresses listed Echo360 Lecture01_to_merge objectthe right join retains 243 student email addresses listed MCQ_to_merge objectthe full join retains 245 unique student email addresses listed Lecture01_to_merge MCQ_to_merge. inspection, two observations Lecture01_to_merge missing values (NA) email_address correspond two \"extra\" observations Lecture01_MCQ_full compared Lecture01_MCQ_right.","code":"\nLecture01_to_merge <- Lecture01 %>% \n  select(user_name,email_address,total_views,total_view_time,average_view_time,last_viewed)\nMCQ_to_merge <- MCQ %>% \n  select(first_name,surname,id_number,email_address,quiz_mcq_summative_assessment_real,quiz_mcq_summative_assessment_perc)\nLecture01_MCQ_left <- left_join(Lecture01_to_merge,MCQ_to_merge,by = \"email_address\")\nLecture01_MCQ_right <- right_join(Lecture01_to_merge,MCQ_to_merge,by = \"email_address\")\nLecture01_MCQ_full <- full_join(Lecture01_to_merge,MCQ_to_merge,by = \"email_address\")\ndim(Lecture01_to_merge)\ndim(MCQ_to_merge)\n\nnames(Lecture01_to_merge)\nnames(MCQ_to_merge)\n\ndim(Lecture01_MCQ_left)\ndim(Lecture01_MCQ_right)\ndim(Lecture01_MCQ_full)\n\nnames(Lecture01_MCQ_left)\nnames(Lecture01_MCQ_right)\nnames(Lecture01_MCQ_full)## [1] 67  6\n## [1] 243   6\n## [1] \"user_name\"         \"email_address\"     \"total_views\"      \n## [4] \"total_view_time\"   \"average_view_time\" \"last_viewed\"      \n## [1] \"first_name\"                         \"surname\"                           \n## [3] \"id_number\"                          \"email_address\"                     \n## [5] \"quiz_mcq_summative_assessment_real\" \"quiz_mcq_summative_assessment_perc\"\n## [1] 67 11\n## [1] 243  11\n## [1] 245  11\n##  [1] \"user_name\"                          \"email_address\"                     \n##  [3] \"total_views\"                        \"total_view_time\"                   \n##  [5] \"average_view_time\"                  \"last_viewed\"                       \n##  [7] \"first_name\"                         \"surname\"                           \n##  [9] \"id_number\"                          \"quiz_mcq_summative_assessment_real\"\n## [11] \"quiz_mcq_summative_assessment_perc\"\n##  [1] \"user_name\"                          \"email_address\"                     \n##  [3] \"total_views\"                        \"total_view_time\"                   \n##  [5] \"average_view_time\"                  \"last_viewed\"                       \n##  [7] \"first_name\"                         \"surname\"                           \n##  [9] \"id_number\"                          \"quiz_mcq_summative_assessment_real\"\n## [11] \"quiz_mcq_summative_assessment_perc\"\n##  [1] \"user_name\"                          \"email_address\"                     \n##  [3] \"total_views\"                        \"total_view_time\"                   \n##  [5] \"average_view_time\"                  \"last_viewed\"                       \n##  [7] \"first_name\"                         \"surname\"                           \n##  [9] \"id_number\"                          \"quiz_mcq_summative_assessment_real\"\n## [11] \"quiz_mcq_summative_assessment_perc\""},{"path":"Echo_combine.html","id":"merging-data-where-each-student-may-appear-more-than-once-in-the-echo360-data","chapter":"8 Combining Echo360 data with other sources of data in R with tidyverse","heading":"8.2 Merging data where each student may appear more than once in the Echo360 data","text":"interested combining Echo360 data multiple lectures (thus students may appear ) together another data object students appear (checklist MCQ data described ) can still use left_join() function. Despite function including argument multiple \"handling rows x multiple matches y\", argument relevant scenario since Echo360 data x argument multiple rows student can merged another data source y argument (within student appears ) one ways shown previously:First select variables interest data object, Lectures_010203 contains Echo360 data first three lectures.NB. students may appear , necessary include variable distinguishes repeated appearances. case variable media_name identifies lecture Echo360 data .Second, specify type 'join', case one :Third, check input output data objects expect.note following property:left join retains 145 observations listed Echo360 Lecture010203_to_merge object combines (single) corresponding observation MCQ_to_merge","code":"\nLecture010203_to_merge <- Lectures_010203 %>% \n  select(user_name, email_address,media_name,total_views,\n         total_view_time,average_view_time,last_viewed)\n\nMCQ_to_merge <- MCQ %>% \n  select(first_name,surname,id_number,email_address,\n         quiz_mcq_summative_assessment_real,quiz_mcq_summative_assessment_perc)\nLecture010203_MCQ_left <- left_join(Lecture010203_to_merge,MCQ_to_merge,by = \"email_address\")\ndim(Lecture010203_to_merge)\ndim(MCQ_to_merge)\n\nnames(Lecture010203_to_merge)\nnames(MCQ_to_merge)\n\ndim(Lecture010203_MCQ_left)\nnames(Lecture010203_MCQ_left)## [1] 145   7\n## [1] 243   6\n## [1] \"user_name\"         \"email_address\"     \"media_name\"       \n## [4] \"total_views\"       \"total_view_time\"   \"average_view_time\"\n## [7] \"last_viewed\"      \n## [1] \"first_name\"                         \"surname\"                           \n## [3] \"id_number\"                          \"email_address\"                     \n## [5] \"quiz_mcq_summative_assessment_real\" \"quiz_mcq_summative_assessment_perc\"\n## [1] 145  12\n##  [1] \"user_name\"                          \"email_address\"                     \n##  [3] \"media_name\"                         \"total_views\"                       \n##  [5] \"total_view_time\"                    \"average_view_time\"                 \n##  [7] \"last_viewed\"                        \"first_name\"                        \n##  [9] \"surname\"                            \"id_number\"                         \n## [11] \"quiz_mcq_summative_assessment_real\" \"quiz_mcq_summative_assessment_perc\""},{"path":"Echo_combine.html","id":"illustration-visualizing-merged-data","chapter":"8 Combining Echo360 data with other sources of data in R with tidyverse","heading":"8.3 Illustration: Visualizing merged data","text":"Merging data enables exploration video engagement (recorded Echo360 data) related student performance. example, number times student views video content can combined performance assessment (contained MCQ data ). visualization data can obtained follows:plot can made interactive using plotly() funciton illustrated previous sections.","code":"\nstudent_data <- Lectures_010203 %>% \n  group_by(email_address) %>%\n  summarise(total_views=sum(total_views)) %>%\n  mutate_at('email_address', factor)\n\nstudent_MCQ <- left_join(student_data,MCQ_to_merge,by = \"email_address\")\n\nfig.no_viewed.mcq <- ggplot(student_MCQ, aes(x=total_views, y=quiz_mcq_summative_assessment_perc)) +\n  geom_point()+ \n  labs(title=\"Number of video views vs. Percentage Score in a Multiple Choice Quiz\") + \n  ylab(\"Percentage Score in MCQ\") +\n  xlab(\"No. of video views\") +\n  theme_bw()\n\nfig.no_viewed.mcq"},{"path":"installing-r.html","id":"installing-r","chapter":"A Installing R","heading":"A Installing R","text":"Installing R RStudio usually straightforward. sections explain helpful YouTube video . run serious difficulties (example admin rights machine), purposes workshop recommend using RStudio Cloud time trouble-shoot complex installation issues.","code":""},{"path":"installing-r.html","id":"installing-base-r","chapter":"A Installing R","heading":"A.1 Installing Base R","text":"Install base R. Choose download link operating system (Linux, Mac OS X, Windows).Mac, install latest release newest R-x.x.x.pkg link (legacy version older operating system). may also need install XQuartz able use visualisation packages.installing Windows version, choose \"base\" subdirectory click download link top page.using Linux, choose specific operating system follow installation instructions.","code":""},{"path":"installing-r.html","id":"installing-rstudio","chapter":"A Installing R","heading":"A.2 Installing RStudio","text":"Go rstudio.com download RStudio Desktop (Open Source License) version operating system list titled Installers Supported Platforms.","code":""},{"path":"installing-r.html","id":"installing-rtools","chapter":"A Installing R","heading":"A.3 Installing RTools","text":"using Windows, install R, also install RTools; use \"recommended\" version highlighted near top list. RTools used installing loading packages. can get started without installing RTools, problems installing loading packages, first thing try.RTools require put \"PATH\". instructions can seem bit vague - easiest way open RStudio, run code console:done , restart R clicking Session - Restart R run code console give path RTools installation:","code":"\nwrite('PATH=\"${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}\"', file = \"~/.Renviron\", append = TRUE)\nSys.which(\"make\")##                               make \n## \"C:\\\\rtools40\\\\usr\\\\bin\\\\make.exe\""},{"path":"installing-r.html","id":"rstudio-settings","chapter":"A Installing R","heading":"A.4 RStudio Settings","text":"settings fix immediately updating RStudio. Go Global Options... Tools menu (⌘,), General tab, uncheck box says Restore .RData workspace startup. keep things around workspace, things get messy, unexpected things happen. always start clear workspace. also means never want save workspace exit, set Never. thing want save scripts.may also want change appearance code. Different fonts themes can sometimes help visual difficulties dyslexia.\nFigure .1: RStudio General Appearance settings\nmay also want change settings Code tab. example, Lisa DeBruine prefers two spaces instead tabs code likes able see whitespace characters. matter personal preference.\nFigure .2: RStudio Code settings\n","code":""},{"path":"installing-r.html","id":"installing-latex","chapter":"A Installing R","heading":"A.5 Installing LaTeX","text":"can install LaTeX typesetting system produce PDF reports RStudio. Without additional installation, able produce reports HTML PDF. generate PDF reports, additionally need install tinytex (Xie, 2023) run following code:","code":"\n# run this in the console\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()"},{"path":"installing-r.html","id":"updating-r","chapter":"A Installing R","heading":"A.6 Updating R, RStudio, and packages","text":"time--time, updated versions R, RStudio, packages use (e.g., ggplot) become available. Remember separate, different process come different considerations. recommend updating latest version start new project. definitely recommend updating middle project middle semester bring advice based personal experience pain.","code":""},{"path":"installing-r.html","id":"updating-rstudio","chapter":"A Installing R","heading":"A.7 Updating RStudio","text":"RStudio easiest component update. Typically, updates RStudio affect code, instead add new features, like spell-check upgrades RStudio can . usually little downside updating RStudio easy .Click Help - Check updates\nFigure .3: Updating RStudio\nupdate available, prompt download can install usual.","code":""},{"path":"installing-r.html","id":"updating-r-1","chapter":"A Installing R","heading":"A.8 Updating R","text":"Finally, may also wish update R . key thing aware update R, just download latest version website, lose packages.","code":""},{"path":"installing-r.html","id":"windows","chapter":"A Installing R","heading":"A.8.1 Windows","text":"easiest way update R Windows cause huge headache use installr package. use updateR() function, series dialogue boxes appear. fairly self-explanatory full step--step guide available use installr, important bit select \"Yes\" asked like copy packages older version R.","code":"\n# Install the installr package\ninstall.packages(\"installr\")\n\n# Run the update function\ninstallR::updateR()"},{"path":"installing-r.html","id":"mac","chapter":"A Installing R","heading":"A.8.2 Mac","text":"Mac, can use updateR package. need install GitHub. asked type system password (use log computer) console pane. relevant, ask want restore packages new major version.","code":"\n# install from github\ndevtools::install_github(\"AndreaCirilloAC/updateR\")\n\n# update your R version, you will need your system password\nupdateR::updateR()"},{"path":"installing-r.html","id":"updating-packages","chapter":"A Installing R","heading":"A.9 Updating packages","text":"completely new R installed packages yet, section make great deal sense, just remember can come back future.Package developers occasionally release updates packages. typically add new functions package, fix amend existing functions. aware package updates may cause previous code stop working. tend happen minor updates packages, occasionally major updates, can serious issues developer made fundamental changes code works. reason, recommend updating packages beginning academic year (semester) - assessment deadline just case!update individual package, easiest way use install.packages() function, always installs recent version package.update multiple packages, indeed packages, RStudio provides helpful tools. Click Tools - Check Package Updates. dialogue box appear can select packages wish update. aware select packages, may take time unable use R whilst process completes.\nFigure .4: Updating packages RStudio\n","code":"\ninstall.packages(\"tidyverse\")"},{"path":"installing-r.html","id":"package-install-troubleshooting","chapter":"A Installing R","heading":"A.10 Troubleshooting","text":"Occasionally, might problem packages seemingly refuse update install. Emily, rlang vctrs cause end trouble. packages likely every explicitly load, required beneath surface R things like knit Markdown files etc.","code":""},{"path":"installing-r.html","id":"non-zero-exit-status","chapter":"A Installing R","heading":"A.10.1 Non-zero exit status","text":"try update package get error message says something like Warning install.packages : installation package ‘vctrs’ non-zero exit status perhaps Error loadNamespace(, c(lib.loc, .libPaths()), versionCheck = vI[[]]) :  namespace 'rlang' 0.4.9 loaded, >= 0.4.10 required one solution found manually uninstall package, restart R, install package new, rather trying update existing version. installr package also useful function uninstalling packages.","code":"\n# Load installr\nlibrary(installr)\n\n# Uninstall the problem package\nuninstall.packages(\"package_name\")\n\n# Then restart R using session - restart R\n# Then install the package fresh\n\ninstall.packages(\"package\")"},{"path":"installing-r.html","id":"cannot-open-file","chapter":"A Installing R","heading":"A.10.2 Cannot open file","text":"may get following error trying install packages :Error install packages : open file 'C:/.....': Permission deniedThis usually indicates permissions problem writing default library (folder packages kept ). Sometimes means need install R RStudio administrator run administrator.One fix may change library location using following code (check \"C:/Program Files/R\" version instead \"R-3.5.2\"):works can install packages, set library path permanently:Install usethis packageRun usethis::edit_r_profile() console; open blank filePaste file (version ): .libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))Save close fileRestart R changes take effectThe code .Rprofile now run every time start R.","code":"\n# change the library path\n.libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))"},{"path":"symbols.html","id":"symbols","chapter":"B Symbols","heading":"B Symbols","text":"\nFigure B.1: Image James Chapman/Soundimals\n","code":""},{"path":"conventions.html","id":"conventions","chapter":"C Conventions","heading":"C Conventions","text":"book use following conventions:Generic code: list(number = 1, letter = \"\")Highlighted code: dplyr::slice_max()File paths: data/sales.csvR Packages: tidyverseFunctions: paste()Strings: \"psyTeachR\"Numbers: 100, 3.14Logical values: TRUE, FALSEGlossary items: ordinalCitations: Wickham (2023)Internal links: Chapter 1External links: R Data ScienceMenu/interface options: New File...","code":""},{"path":"conventions.html","id":"webexercises","chapter":"C Conventions","heading":"C.1 Webexercises","text":"See webexercises details use materials.Type integer: going learn lot: TRUEFALSEWhat p-value?\n\nprobability null hypothesis truethe probability observed (extreme) data, assumption null-hypothesis truethe probability making error conclusion\nfound hidden text!","code":"\nprint(\"You found some hidden code!\")## [1] \"You found some hidden code!\""},{"path":"conventions.html","id":"alert-boxes","chapter":"C Conventions","heading":"C.2 Alert boxes","text":"Informational asides.Notes warn something.Notes things cause serious errors.Try .","code":""},{"path":"conventions.html","id":"code-chunks","chapter":"C Conventions","heading":"C.3 Code Chunks","text":"","code":"\n# code chunks\npaste(\"Applied\", \"Data\", \"Skills\", 1, sep = \" \")## [1] \"Applied Data Skills 1\"```{r setup, message = FALSE}\n# code chunks with visible r headers\nlibrary(tidyverse)```"},{"path":"conventions.html","id":"glossary","chapter":"C Conventions","heading":"C.4 Glossary","text":"","code":""},{"path":"license.html","id":"license","chapter":"License","heading":"License","text":"book licensed Creative Commons Attribution-ShareAlike 4.0 International License (CC--SA 4.0). free share adapt book. must give appropriate credit (DeBruine, 2021), provide link license, indicate changes made. adapt material, must distribute contributions license original.","code":""},{"path":"references-1.html","id":"references-1","chapter":"References","heading":"References","text":"","code":""}]
