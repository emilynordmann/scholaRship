[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"scholaRship collection tutorials engage scholarship teaching learning using quantitative methods R. written Learning, Teaching, Scholarship focused academics: Emily Nordmann, James Bartlett Gaby Mahrholz School Psychology Neuroscience, University Glasgow, Mitchum Bock, Eilidh Jack, Craig Alexander, School Mathematics Statistics, University Glasgow, Jill MacKay, University Edinburgh.book considered living document. tutorials added time developed content structure new existing tutorials may updated.Contact: Emily Nordmann (Emily.Nordmann@glasgow.ac.uk)","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"Getting started scholarship teaching learning can difficult. majority academics whose subject expertise involve learning teaching, first hurdle figuring questions can ask answer (indeed interested ) can toughest one push past.settled area inquiry, may find appropriate methodologies investigate questions ones trained . quantitatively-minded researchers, availability data can feel like simultaneous feast famine - may access huge amounts data learning analytics standard student records able use almost none research purposes due need opt-consent. consent obtained, may small, non-representative samples, non-random attrition, /concerns making data openly available.Finally, data can seriously messy: missing data, data multiple sources different structures labels, data different academic years course structures assessments changed, anonymised data, aggregated data.sounds familiar, book .tutorial book contain:short summary evidence-base problem investigation promote engagement SoTL literature;Real1 data drawn commonly available sources Moodle, Turnitin, Microsoft Forms, Echo360;walkthrough clean wrangle data using predominantly tidyverse approach;walkthrough analyse interpret, analysis, alongside honest discussion limitations approach used.","code":""},{"path":"intro.html","id":"planned-tutorials","chapter":"1 Introduction","heading":"1.1 Planned tutorials","text":"following tutorials planned happy take suggestions - please e-mail Emily.Nordmann@glasgow.ac.uk.Analysing impact whether students check feedback Turnitin Feedback Studio subsequent assessment performance.Creating exam board moderation reports using RUsing Moodle logs predict engagement retention","code":""},{"path":"intro.html","id":"expectations-of-prior-knowledge","chapter":"1 Introduction","heading":"1.2 Expectations of prior knowledge","text":"","code":""},{"path":"intro.html","id":"r-and-rstudio","chapter":"1 Introduction","heading":"1.2.1 R and RStudio","text":"Minimal prior knowledge R RStudio assumed throughout book. functions code used explained, however, assume reader understands :Install R RStudioInstall R RStudioNavigate RStudioNavigate RStudioSet working directory appropriatelySet working directory appropriatelyInstall load packagesInstall load packagesWrite execute codeWrite execute codeEach chapter start overview expected prior knowledge along resources recap necessary.","code":""},{"path":"intro.html","id":"research-methods-and-statistics","chapter":"1 Introduction","heading":"1.2.2 Research methods and statistics","text":"assume basic level competency research methods statistics. However, also recognise many researchers still less familiar modern approaches mixed effects models provide appropriate level explanation resources necessary.","code":""},{"path":"creating-synthetic-datasets.html","id":"creating-synthetic-datasets","chapter":"2 Creating synthetic datasets","heading":"2 Creating synthetic datasets","text":"Lead author: James Bartlett","code":""},{"path":"creating-synthetic-datasets.html","id":"introduction","chapter":"2 Creating synthetic datasets","heading":"2.1 Introduction","text":"scholarship teaching learning, might analyse data sources like virtual learning environments student characteristics. Student agreements normally allow educators analyse data purposes internally improving education experiences, want disseminate findings wider audience, need gain ethical approval students use data additional purposes. Even ethical approval, might additional concerns around privacy anonymity data contain sensitive personal information. data sharing practices become routine, important contribute scholarly progress maintaining participant anonymity. tutorial, demonstrate can create synthetic data sets strategy sharing primary data present ethical issues.Synthetic data mimics properties original data closely possible, retaining distribution grades sample. means can retain underlying statistical properties variables, \"individual participants\" longer represent original cases, meaning risk identification much lower. R package synthpop (Nowok et al., 2016) creates synthetic data sampling probability distribution best suited variables data set. precise technical details beyond scope tutorial, refer interested readers Nowok et al. information.important evaluate project individually judge whether can ethically share research data, Meyer (2018) provides practical tips:Wherever possible, get informed consent participants retain share data.Wherever possible, get informed consent participants retain share data.promise destroy never share data. Often researchers reassure ethical review boards data never shared beyond research team, providing appropriate , can simply ask participants consent data shared.promise destroy never share data. Often researchers reassure ethical review boards data never shared beyond research team, providing appropriate , can simply ask participants consent data shared.thoughtful considering risks re-identification.thoughtful considering risks re-identification.Assuming receive consent data sharing, likelihood identifying individual participants motivates synthetic data tutorial. Even exclude identifiable information like names email addresses, participants re-identified combining common demographic variables ethnicity, date birth, sex might want share related research question. risk re-identification may even higher participants know . example, students might know studied cohort able identify data set recognise participants relative details. Therefore, want share data part scholarship receive consent participants , important think whether variables want share used identify participants whether providing synthetic data set original data appropriate. See Meyer (2018) details ethical data sharing.tutorial, demonstrate create synthetic data set using synthpop package R. Quintana (2020) previously written accessible tutorial package recommend focuses biomedical science data. tutorial use data student's academic procrastination resonate closely scholarship teaching learning.","code":""},{"path":"creating-synthetic-datasets.html","id":"dunn-2014-replication","chapter":"2 Creating synthetic datasets","heading":"2.1.1 Dunn (2014) Replication","text":"data set use unpublished replication attempt Dunn (2014). Dunn wanted understand impact motivation statistics anxiety students' academic procrastination. sample included graduate students online course. wanted replicate study see observe similar findings response COVID-19 pandemic face--face students forced study online. Dunn replication perfect demonstrate synthetic data set includes range data types included open data consent forms, meaning can openly compare properties data set.research question : intrinsic motivation, academic self-regulation, statistics anxiety influence students' passive procrastination? expected intrinsic motivation self-regulation negative predictors, whereas expected statistics anxiety positive predictor passive procrastination. study included following variables:General strategies learning (GSL; GSL) - modified subscale Motivated Strategies Learning Questionnaire (MSLQ) measures academic self-regulation. Measured 1-7 scale, higher values meaning higher self-regulation.General strategies learning (GSL; GSL) - modified subscale Motivated Strategies Learning Questionnaire (MSLQ) measures academic self-regulation. Measured 1-7 scale, higher values meaning higher self-regulation.Intrinsic motivation (Intrinsic) - subscale MSLQ measures intrinsic motivation inherent joy people find task. Measured 1-7 scale, higher values meaning higher intrinsic motivation.Intrinsic motivation (Intrinsic) - subscale MSLQ measures intrinsic motivation inherent joy people find task. Measured 1-7 scale, higher values meaning higher intrinsic motivation.Statistical Anxiety Rating Scale (STARS; STARS) - measure statistics anxiety included statistics test class anxiety subscale. Measured 1-5 scale, higher values meaning higher statistics anxiety.Statistical Anxiety Rating Scale (STARS; STARS) - measure statistics anxiety included statistics test class anxiety subscale. Measured 1-5 scale, higher values meaning higher statistics anxiety.Procrastination Assessment Scale Students (PASS; PASS) - measure passive procrastination keeping writing assignments studying exams. Measured 1-5 scale higher scores meaning greater procrastination, scale uses sum items creating possible range 6-30.Procrastination Assessment Scale Students (PASS; PASS) - measure passive procrastination keeping writing assignments studying exams. Measured 1-5 scale higher scores meaning greater procrastination, scale uses sum items creating possible range 6-30.","code":""},{"path":"creating-synthetic-datasets.html","id":"prior-knowledge","chapter":"2 Creating synthetic datasets","heading":"2.1.2 Prior knowledge","text":"complete tutorial need following prior knowledge - recap links point additional materials cover detail):install load packages R (recap)set working directory run code R (recap)() use pipe %>% operator (recap)conceptual understanding linear regression (recap)","code":""},{"path":"creating-synthetic-datasets.html","id":"synthpop-setup","chapter":"2 Creating synthetic datasets","heading":"2.1.3 Set-up","text":"demonstrate synthpop package, explore original data set modelling.Download Dunn-replication.csv save folder named data working directory.run code load required packages (may require installation) data.","code":"\nlibrary(tidyverse) # Collection of functions for data wrangling and visualisation\nlibrary(performance) # Functions for assessing statistical models\nlibrary(effectsize) # Functions for calculating effect sizes\nlibrary(synthpop) # Package to create our synthetic data set later\n\n# Two functions to make prettier html tables\nlibrary(knitr)\nlibrary(kableExtra)\n\nreal_data <- read_csv(\"data/Dunn-replication.csv\") %>% \n  select(-user_id, -SelfEfficacy, -HelpSeeking) # omit some columns we don't need"},{"path":"creating-synthetic-datasets.html","id":"original-dataset-analyses","chapter":"2 Creating synthetic datasets","heading":"2.2 Original dataset analyses","text":"address research question using model Dunn (2014), want use linear regression PASS outcome three predictors GSL, intrinsic motivation, statistics anxiety.function lm() constructs linear modelThe formula takes form outcome ~ predictor1 + predictor2 etc.first save results regression object model pass object summary() function explore model results:replication different setting, results pretty close Dunn (2014). Overall, significant model explains 18% (adj. \\(R^2\\) = 0.16) variance passive procrastination. Similar Dunn, GSL significant negative predictor passive procrastination, 1 unit increase GSL associated 1.77 decrease passive procrastination. Intrinsic motivation non-significant weak negative predictor. Interestingly, statistics anxiety significant predictor Dunn, significant positive predictor replication, 1 unit increase STARS associated 1.69 increase passive procrastination. words, passive procrastination tended increase higher levels statistics anxiety lower levels GSL.values original units measurement, can also report standardised coefficients express values standard deviations, consistent Dunn reported results. can standardising predictors entering model, can use handy standardize_parameters() function effectsize package get 95% confidence intervals . , save results table add estimates Dunn (2014) comparison.code:Calculates standardized coefficients saves object named tableCreates vector coefficients original Dunn studyAdds vector column tableMakes nice looking table using kable - note table output might look slightly different bookdown package use write book applies additional formatting.\nTable 2.1: Standardised Beta coefficients 95% confidence interval (CI) replication data compared Dunn (2014).\ncomparison, coefficient STARS larger Dunn, coefficients GSL intrinsic motivation smaller. findings largely consistent though, coefficients direction within 95% CI estimates. Despite intrinsic motivation significant predictor, findings consistent hypotheses largely replicate findings Dunn.final check, important make sure model results consistent assumptions linear regression. great package called performance includes helper functions like check_model(). can concisely check model assumptions:Although bottom right plot normality residuals quite capturing peak normal distribution, nothing suggest warning signs model.findings assumptions order, know real data set telling us. Now, time create synthetic data set using synthpop package see close replicates features.","code":"\n# One outcome, three predictors\nmodel <- lm(PASS ~ GSL + Intrinsic + STARS,\n            data = real_data)\n\n(model_results <- summary(model)) # surround with brackets to both print and save## \n## Call:\n## lm(formula = PASS ~ GSL + Intrinsic + STARS, data = real_data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -14.8588  -3.4830   0.0803   3.2853   9.9100 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  23.6386     3.0068   7.862  3.2e-12 ***\n## GSL          -1.7677     0.4698  -3.762 0.000275 ***\n## Intrinsic    -0.1780     0.5160  -0.345 0.730767    \n## STARS         1.6926     0.5961   2.840 0.005408 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.405 on 107 degrees of freedom\n## Multiple R-squared:  0.1815, Adjusted R-squared:  0.1586 \n## F-statistic: 7.911 on 3 and 107 DF,  p-value: 8.154e-05\n#1. Use the standardize_parameters function\n#2. Select rows 2-4, ignoring the intercept\n#3. Save in a data frame\n#4. Drop the 95% CI column as it's just descriptive \n\ntable <- standardize_parameters(model) %>%\n  slice(2:7) %>%\n  data.frame() %>%\n  select(-CI)\n\n# Manually save values from the original Dunn study for comparison\ndunn <- c(-0.55, # GSL\n          -0.17, # Intrinsic\n          0.15) # STARS\n\n# Add these values to our table above\ntable$dunn <- dunn\n\n# Use the kable and kableextra functions to create a nicer looking table \nkable(table, \n      digits = 2, \n      format = \"html\", \n      col.names = c(\"Parameter\", \"Beta\", \"Lower 95% CI\", \"Upper 95% CI\", \"Beta from Dunn (2014)\"), \n      caption = \"Standardised Beta coefficients and their 95% confidence interval (CI) in the replication data compared to Dunn (2014).\") %>% \n  kable_styling()\ncheck_model(model)"},{"path":"creating-synthetic-datasets.html","id":"synthesise-with-synthpop","chapter":"2 Creating synthetic datasets","heading":"2.3 Synthesise with synthpop","text":"synthpop package (Nowok et al., 2016) aims mimic observed data preserve relationship variables. authors developed package work around limitations working vast data coming national statistical agencies. population level data sets can provide important insights, granularity data rightly leads privacy concerns identifiable participants , restricting access data. Working higher education data, can face similar concerns. access student level data can provide important insights, often access share data confidentiality constraints. synthetic data can useful.Packages like synthpop attempt reconstruct data set sampling probability distributions relevant type data working . means properties variables relationships retained closely possible. Sometimes less accurate, explore factors keep mind later .","code":""},{"path":"creating-synthetic-datasets.html","id":"preparing-the-data-set","chapter":"2 Creating synthetic datasets","heading":"2.3.1 Preparing the data set","text":"creating synthetic data set important check data determine number type variables, well much missing data () can using codebook.syn() function.Whilst output might look useful, actually missing lot information dataset individual variables quite right format.first step convert object data frame. used \"tidyverse\" family packages, data saved something called tibble. Tibbles similar data frames, tweaks work better tidyverse (see tibble chapter R Data Science book; Wickham & Grolemund, 2017). Although tibbles can useful within tidyverse framework, can occasionally cause problems. found hard way writing tutorial try use data without converting data frame, get cryptic error message Error tab.obs[[]] + tab.syn[[]] : non-conformable array took us hour figure . reading forum highlighting tibbles caused similar problem another package tried converting data frame first solved issue.second step ensure character variables set factors whilst data columns text data, informative text represents different categories. creating synthetic data set, function works probability distribution using original data, setting characters factors establishes number unique groups per variable.Now, repeat codebook function, get added details informing us number levels per variable factors data range numeric variables. data set, missing data, , central columns inform us number percent missing values, also something can estimated part synthetic data function.","code":"\ncodebook.syn(real_data)## $tab\n##           variable     class nmiss perctmiss ndistinct details\n## 1              age   numeric     0         0        16        \n## 2           gender character     0         0         4        \n## 3 degree_programme character     0         0         4        \n## 4              GSL   numeric     0         0        24        \n## 5        Intrinsic   numeric     0         0        18        \n## 6            STARS   numeric     0         0        25        \n## 7             PASS   numeric     0         0        20        \n## \n## $labs\n## NULL\n# Resave real_data as a data frame instead of a tibble\nreal_data <- as.data.frame(real_data)\n\n# Convert gender to a factor\nreal_data$gender <- as.factor(real_data$gender)\n\n# Convert degree programme to a factor\nreal_data$degree_programme <- as.factor(real_data$degree_programme)\ncodebook.syn(real_data)## $tab\n##           variable   class nmiss perctmiss ndistinct           details\n## 1              age numeric     0         0        16    Range: 18 - 50\n## 2           gender  factor     0         0         4 See table in labs\n## 3 degree_programme  factor     0         0         4 See table in labs\n## 4              GSL numeric     0         0        24  Range: 1.6 - 6.8\n## 5        Intrinsic numeric     0         0        18 Range: 2.5 - 6.75\n## 6            STARS numeric     0         0        25  Range: 1.875 - 5\n## 7             PASS numeric     0         0        20     Range: 6 - 30\n## \n## $labs\n## $labs$gender\n##                  label\n## 1               Female\n## 2                 Male\n## 3           Non-binary\n## 4 Prefer not to answer\n## \n## $labs$degree_programme\n##   label\n## 1    BA\n## 2   BSc\n## 3    MA\n## 4   MSc"},{"path":"creating-synthetic-datasets.html","id":"subset-the-data","chapter":"2 Creating synthetic datasets","heading":"2.3.2 Subset the data","text":"Previously, mentioned synthpop match features original data closely possible, can less accurate. One factor associated ratio number participants number variables data set. synthetic data set tries retain association variables, larger number variables, combinations function must consider. enough participants, synthpop provide warning number participants recommends based number variables data set. 100 + 10 * number variables. , two variables, recommend 120 participants, eight variables 180 participants etc. fewer participants recommended, estimation process might less precise.demonstration, first create limited data set two variables show works, scale full data set. following code, set seed make analyses reproducible. function set.seed() controls random number generator - using functions use randomness, running set.seed() ensure get result time run function (many cases may want ). estimation synthetic data set based simulations, set seed get results tutorial.know GSL strongest predictor original data, limit results just predicting academic procrastination GSL create synthetic data set using syn() function.Now, synthetic data set saved object environment, synthpop includes function save new data current working directory. following function takes synthetic data object, like file called, file type want saved :ran code save synthetic data set, see three new files working directory. . RData object can reload R. choosing .csv file, also spreadsheet containing data analyse software read R. Finally, .txt file information synthetic data process like name original data file, seed used, variables.","code":"\n# Set a seed for reproducible analyses\nmy_seed <- 2018\n\n# limit the real data to just two variables \nshort_data <- real_data %>% \n  select(PASS, GSL)\n\n# Save the synthetic data using the reproducible seed from above\nsynth_data <- syn(short_data, \n                   seed = my_seed)## CAUTION: Your data set has fewer observations (111) than we advise.\n## We suggest that there should be at least 120 observations\n## (100 + 10 * no. of variables used in modelling the data).\n## Please check your synthetic data carefully with functions\n## compare(), utility.tab(), and utility.gen().\n## \n## \n## Synthesis\n## -----------\n##  PASS GSL\nwrite.syn(synth_data, \n          filename = \"synthetic_Dunn_data\", \n          filetype = \"csv\")## Synthetic data exported as csv file(s).\n## Information on synthetic data written to\n##   C:/Users/staff/OneDrive - University of Glasgow/Teaching/psyteachR/scholaRship/book/synthesis_info_synthetic_Dunn_data.txt"},{"path":"creating-synthetic-datasets.html","id":"compare-data-sets","chapter":"2 Creating synthetic datasets","heading":"2.3.3 Compare data sets","text":"Now synthetic data, synthpop useful functions compare new data observed data. example, can compare distribution values two data sets see well reconstructed values. stat argument, can choose either \"counts\" \"percents\" show histogram frequency percentage values, depending prefer interpret.can see two histograms limited selection two variables. dark blue, frequency observations observed data, light blue, frequency observations synthetic data. perfect match, observed values higher synthetic, synthetic values higher observed. never going perfect, trying capture features closely possible, different underlying distributions can produce relationships variables.histograms, default settings utility measures. statistics summarise closely synthetic data compares observed data, assuming synthesis model correct. three default results pMSE (propensity score mean-squared error), S_pMSE (standardised measure pMSE), df (degrees freedom Chi-Square tests). many utility measures can request using utility.stats argument. explore measures available, read documentation entering help(\"utility.tab\") console.want fall statistics rabbit role, refer Raab et al. (2021) Snoke et al. (2018) test different utility measures. brief overview, general/global specific/narrow utility measures. Global measures attempt capture well synthesis process reconstructs relationships across whole data set rather result one specific statistical model. output , pMSE one measure. Propensity scores work combining two data sets calculating probability observation comes synthetic data. means propensity score mean-squared error (pMSE) measures error associated process, higher values meaning greater error. closer 0 better, means highest utility hard distinguish two data sets. away 0, easier distinguish two data sets. values small, nothing worry .hand, narrow measures compare models data sets, closely regression coefficients replicated. lm.synds() function synthpop creates linear model using synthetic data sets, can refit using observed data compare different .following code, first create simple linear regression model using PASS outcome GSL single predictor. save object use compare() function .bunch output dissect , start plot visual overview. Similar last compare() output, observed data displayed dark blue synthetic data displayed light blue. plot shows point estimate confidence interval regression coefficient Z value. Visually, looks pretty good. synthetic data estimate slightly smaller, confidence intervals largely overlap.Now initial impression, break output table. informing model fitted, summary model coefficients synthetic observed data, difference , much confidence intervals overlap. intercept, difference -0.46 GSL coefficient, difference 0.10. quite small, keep mind original units measurement, need interpret differences relative measures. Reinforcing visual interpretation, confidence intervals largely overlap, approximately 93% coverage parameters.Finally, Chi-Square test assumes synthetic data model compatible observed data model. essence, null hypothesis difference two models. Acknowledging limitations null hypothesis significance testing, smaller p-value, incompatible two models . p-value close 1, suggesting significant difference two models. p-value much smaller, traditional threshold .05, suspect synthetic data process capture relationships observed data.summarise smaller selection procedure, limited data set two variables: predicting PASS GSL scores. created synthetic data set using synthpop package explored general narrow utility measures. types measures showed synthetic data set good job capturing properties observed data. means share synthetic data set concerns sharing original observed data set. Just remember clearly label synthetic data set fake data set inform readers replaced observed data.","code":"\ncompare(synth_data, # synthetic data object\n        short_data, # original data\n        stat = \"counts\") # Choice of counts or percents## \n## Comparing counts observed with synthetic\n## \n## \n## Selected utility measures:\n##          pMSE   S_pMSE df\n## PASS 0.000207 0.091919  4\n## GSL  0.002052 0.911044  4\n# Linear model equivalent in synthetic data, one outcome and one predictor\ns_lm <- lm.synds(PASS ~ GSL,\n                 data = synth_data)\n\ncompare(s_lm, # saved object from above for lm applied to synthetic data\n        short_data) # original data## \n## Call used to fit models to the data:\n## lm.synds(formula = PASS ~ GSL, data = synth_data)\n## \n## Differences between results based on synthetic and observed data:\n##             Synthetic  Observed       Diff Std. coef diff CI overlap\n## (Intercept) 27.856804 28.319159 -0.4623549     -0.2619675  0.9331703\n## GSL         -1.497014 -1.598289  0.1012751      0.2409324  0.9385365\n## \n## Measures for one synthesis and 2 coefficients\n## Mean confidence interval overlap:  0.9358534\n## Mean absolute std. coef diff:  0.25145\n## \n## Mahalanobis distance ratio for lack-of-fit (target 1.0): 0.04\n## Lack-of-fit test: 0.07152817; p-value 0.9649 for test that synthesis model is\n## compatible with a chi-squared test with 2 degrees of freedom.\n## \n## Confidence interval plot:"},{"path":"creating-synthetic-datasets.html","id":"full-data-set","chapter":"2 Creating synthetic datasets","heading":"2.3.4 Full data set","text":"Now taken close look limited selection variables, scale things see synthpop looks like variable types relationships consider. go back using original real_data file seven variables. first step using syn() function create new synthetic data set. already processed data set reducing number variables, remember check using tibble whether character variables need converting factors.working amount data 111 participants trying reconstruct variables. Remember synthetic data process based sampling appropriate probability distribution, package recommends minimum number participants (100 + 10 * number predictors). smaller selection, close recommendation two predictors least 120 participants. However, larger selection, away recommendation now least 170 participants. Keep mind checking utility measures.Next, compare two data sets see close reconstructed variables:output longer since now seven variables reconstruct instead smaller selection two. can also see character variables look like since smaller selection contained two numeric variables. demographic information omitted smaller selection, can now see age, gender, participant's degree programme., variables interest use next multiple linear regression, using PASS (academic procrastination) outcome three predictors intrinsic motivation, GSL (general strategies learning), STARS (statistics test anxiety). final table general utility measures see well synthetic data set captured features observed data set. Remember pMSE error associated classifying data coming synthetic observed data set, values 0 indicating greater error.use function create linear model synthetic data set, use compare() function see model performs data set narrow utility measures:Starting visual interpretation, synthetic (light blue) observed (dark blue) estimates pretty close. GSL strong negative predictor, STARS moderate positive predictor, intrinsic motivation weak predictor PASS hovering around zero. confidence intervals overlap quite closely smaller selection, look precise statistics moment.Moving table estimates, now four parameters check. coefficients synthetic data set, observed data set, difference . Remember values unstandardised original units measurement, judge differences relative. Compared smaller selection, harder time reconstructing intercept, performance OK three predictors. Supporting visual inspection, confidence interval coverage lower smaller selection, still captures main features. intrinsic motivation interval worst, STARS interval best. Now parameters, can also helpful look mean overlap across confidence intervals, 68% case. poorer performance probably due smaller recommended sample size compared smaller selection.Finally, can look Chi-Square test assumes synthetic data model compatible observed data model. Smaller p-values suggest greater incompatibility two models. larger selection, also statistically significant, suggesting narrow utility performance ideal, inconsistent observed data.","code":"\nsynth_data2 <- syn(real_data, # return to using the original larger data set\n                   seed = my_seed) # use same seed as above ## CAUTION: Your data set has fewer observations (111) than we advise.\n## We suggest that there should be at least 170 observations\n## (100 + 10 * no. of variables used in modelling the data).\n## Please check your synthetic data carefully with functions\n## compare(), utility.tab(), and utility.gen().\n## \n## \n## Synthesis\n## -----------\n##  age gender degree_programme GSL Intrinsic STARS PASS\ncompare(synth_data2, \n        real_data, \n        stat = \"counts\")## \n## Comparing counts observed with synthetic\n## \n## Press return for next variable(s): \n## \n## Selected utility measures:\n##                      pMSE   S_pMSE df\n## age              0.003145 2.792530  2\n## gender           0.000425 0.251701  3\n## degree_programme 0.003070 1.817685  3\n## GSL              0.001869 0.829618  4\n## Intrinsic        0.000741 0.329097  4\n## STARS            0.004231 1.878619  4\n## PASS             0.002649 1.175969  4\n# Second linear model using the full three predictors\ns_lm2 <- lm.synds(PASS ~ GSL + Intrinsic + STARS,\n                 data = synth_data2)\n\n(synth_compare2 <- compare(s_lm2, # New full multiple linear regression model\n        real_data)) # Full observed data file with our 7 variables## \n## Call used to fit models to the data:\n## lm.synds(formula = PASS ~ GSL + Intrinsic + STARS, data = synth_data2)\n## \n## Differences between results based on synthetic and observed data:\n##             Synthetic   Observed       Diff Std. coef diff CI overlap\n## (Intercept) 27.316140 23.6386257  3.6775144      1.2230587  0.6879895\n## GSL         -2.308385 -1.7676992 -0.5406860     -1.1508323  0.7064149\n## Intrinsic   -1.002581 -0.1780281 -0.8245529     -1.5979280  0.5923578\n## STARS        2.280006  1.6926311  0.5873745      0.9853746  0.7486243\n## \n## Measures for one synthesis and 4 coefficients\n## Mean confidence interval overlap:  0.6838466\n## Mean absolute std. coef diff:  1.239298\n## \n## Mahalanobis distance ratio for lack-of-fit (target 1.0): 1.92\n## Lack-of-fit test: 7.683872; p-value 0.1039 for test that synthesis model is\n## compatible with a chi-squared test with 4 degrees of freedom.\n## \n## Confidence interval plot:"},{"path":"creating-synthetic-datasets.html","id":"summary","chapter":"2 Creating synthetic datasets","heading":"2.4 Summary","text":"tutorial, explored create synthetic data sets context scholarship teaching learning. often work sensitive data risk anonymity participants may prevent us accessing sharing data. Open scholarship practices recognise role value sharing research data, synthetic data sets can provide useful compromise scientific ethical responsibilities.created synthetic data sets smaller larger selection variables unpublished replication attempt Dunn (2014). assessing well synthetic data reconstructs observed data, general narrow utility measures. General utility measures include statistics like pMSE (propensity score mean-squared error), whereas narrow utility measures include comparing model parameters like regression coefficient confidence interval. saw performance worse greater mismatch recommended actual sample size, keep package recommendations mind creating synthetic data.tried provide relatively non-technical introduction synthetic data sets common world statistical agencies large granular data sets increase risk re-identification. resources, recommend primer Quintana (2020) included additional sections exploring different levels skew missing data affect synthetic data process. technical details, recommend original package article Nowok et al. (2016) synthpop website includes vignettes list resources learning synthetic data.final remember, always include label instructions informing readers provide synthetic data, mistake real observed data.","code":""},{"path":"creating-synthetic-datasets.html","id":"references","chapter":"2 Creating synthetic datasets","heading":"2.5 References","text":"Meyer, M. N. (2018). Practical Tips Ethical Data Sharing. Advances Methods Practices Psychological Science, 1(1), 131–144. https://doi.org/10.1177/2515245917747656Nowok, B., Raab, G. M., & Dibben, C. (2016). synthpop: Bespoke Creation Synthetic Data R. Journal Statistical Software, 74, 1–26. https://doi.org/10.18637/jss.v074.i11Quintana, D. S. (2020). synthetic dataset primer biobehavioural sciences promote reproducibility hypothesis generation. ELife, 9, e53275. https://doi.org/10.7554/eLife.53275Raab, G. M., Nowok, B., & Dibben, C. (2021). Assessing, visualizing improving utility synthetic data (arXiv:2109.12717). arXiv. https://doi.org/10.48550/arXiv.2109.12717Snoke, J., Raab, G. M., Nowok, B., Dibben, C., & Slavkovic, . (2018). General specific utility measures synthetic data. Journal Royal Statistical Society: Series (Statistics Society), 181(3), 663–688. https://doi.org/10.1111/rssa.12358Wickham, H. & Grolemund, G. (2017). R Data Science. O'Reilly.","code":""},{"path":"moodle-assignment-submission-reports.html","id":"moodle-assignment-submission-reports","chapter":"3 Moodle Assignment submission reports","heading":"3 Moodle Assignment submission reports","text":"Lead author: Emily NordmannPlatforms like Moodle provide extremely detailed assignment submission reports contain data student submitted, extensions, grade. tutorial explains use R create series simple reports data provided Moodle Assignment Submission reports can helpful generating insights student behavior course. tutorial designed Moodle report mind, possible adapt much code reports produced systems.","code":""},{"path":"moodle-assignment-submission-reports.html","id":"set-up","chapter":"3 Moodle Assignment submission reports","heading":"3.1 Set-up","text":"Install (required) load following packagesThe data file simulated assignment submission report 300 students essay submission. Although data simulated, file identical one downloaded Moodle able use code assignment submission reports downloaded Moodle (assuming variable names change institutions!).assignment submission report .xlsx file. lots functions can use R import specific types files, however, import function rio package great wrapper function works pretty much type file avoids remember function need specific file type.first three lines assignment submission report file blank main data table starts line 4, add skip = 3 skip first three lines data import.","code":"\nlibrary(tidyverse) # data wrangling and visualisation\nlibrary(rio) # import data\nlibrary(lubridate) # working with dates\nlibrary(plotly) # interactive plots\nlibrary(psych) # for descriptive stats\ndat <- import(\"https://github.com/emilynordmann/scholaRship/raw/master/book/data/essay_data.xlsx\", skip = 3)"},{"path":"moodle-assignment-submission-reports.html","id":"data-types","chapter":"3 Moodle Assignment submission reports","heading":"3.2 Data types","text":"anything need little work tidy file ensure R data encoded right type. First, check data using str():Based , three things need :Remove - Moodle uses represent missing data replace actual empty cell (.e., NA) using mutate() na_ifConvert date time variables date/time class R currently stored character. Modified (time student last changed file, .e., submitted), Extension, Released currently format day-month-year-hour-minute use dmy_hm function lubridate package.Convert numeric data numeric. variable character text , R read variable character. Moodle used - encode missing values, numeric Turnitin score represented character variable gotten rid - need convert back.lot can data contained file, suggestions insights can generate.","code":"\nstr(dat)## 'data.frame':    300 obs. of  14 variables:\n##  $ #             : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ Username      : chr  \"Name 1\" \"Name 2\" \"Name 3\" \"Name 4\" ...\n##  $ Participant No: chr  \"100000252\" \"100000128\" \"100000244\" \"100000587\" ...\n##  $ Email address : chr  \"myemail@student.ac.uk\" \"myemail@student.ac.uk\" \"myemail@student.ac.uk\" \"myemail@student.ac.uk\" ...\n##  $ ID number     : chr  \"1234818\" \"1234694\" \"1234810\" \"1235153\" ...\n##  $ Groups        : chr  \"group6\" \"group10\" \"group8\" \"group7\" ...\n##  $ Status        : chr  \"submitted\" \"submitted\" \"submitted\" \"submitted\" ...\n##  $ Grade         : chr  \"C3\" \"C1\" \"A4\" \"C1\" ...\n##  $ Turnitin      : chr  \"16\" \"16\" \"12\" \"15\" ...\n##  $ Grader        : chr  \"Marker 4\" \"Marker 4\" \"Marker 4\" \"Marker 3\" ...\n##  $ Modified      : chr  \"16/11/20, 20:14\" \"13/11/20, 08:26\" \"13/11/20, 08:50\" \"12/11/20, 18:05\" ...\n##  $ Released      : chr  \"7/12/20, 12:01\" \"7/12/20, 12:01\" \"7/12/20, 12:01\" \"7/12/20, 12:00\" ...\n##  $ Extension     : chr  \"16/11/20, 12:00\" \"-\" \"-\" \"-\" ...\n##  $ Files         : chr  \"essay1\" \"essay2\" \"essay3\" \"essay4\" ...\ndat_cleaned <- dat %>%\n  mutate(across(where(is.character), ~na_if(., \"-\"))) %>% # replace - with NAs\n  mutate(Modified = dmy_hm(Modified),\n         Extension = dmy_hm(Extension),\n         Released = dmy_hm(Released),\n         Turnitin = as.numeric(Turnitin))"},{"path":"moodle-assignment-submission-reports.html","id":"extensions-and-late-submissions","chapter":"3 Moodle Assignment submission reports","heading":"3.3 Extensions and late submissions","text":"Rates extensions late submission increased significantly covid found helpful look patterns students submit work relative deadline.First, create date time variable contains deadline. use format therefore use dmy_hm function convert information right format R recognises date. deadline state time assignment become late submission - us one minute stated deadline example 1 minute past 12 noon 13th November 2020.also helpful variables store many students course total, many submitted.ways can look extension late submission data. First, calculate number extensions applied simple count. code gives us number extensions day.get total number extensions, adapt code remove NAs calculation sum total calculate percent submissions extension applied:might want something fine-grained need look numbers -time submissions, extensions, late submissions, non-submissions requires bit data wrangling.case_when() allows recode values based multiple -statements. conditions :essay submitted deadline, labelled -time.essay submitted deadline extension deadline, labelled -time extension.\n3 essay submitted deadline extension applied, labelled late.essay submitted labelled non-submission.makes head explode, please know code took half hour multiple errors write got logic correct.can use new categories calculate descriptives:remove non-submissions distribution plot otherwise plotted submitted assignment first opened:can also produce interactive version plot using ggplotly() - can hover bars see counts remove certain groups data:extensions applied Moodle just wanted look difference -time submissions collapse lates extensions one group following:","code":"\ndeadline <-dmy_hm(\"13/11/2020 12:01\")\ntotal_students <- nrow(dat_cleaned)\ntotal_submissions <- dat_cleaned %>%\n  filter(Status == \"submitted\") %>%\n  nrow()\ndat_cleaned %>%\n  count(Extension)\ndat_cleaned %>%\n  count(Extension) %>%\n  filter(!is.na(Extension)) %>% # remove rows that don't have an extension date\n  summarise(total = sum(n),\n            percent = round((total/total_submissions)*100, 2))\ndat_cleaned <- dat_cleaned %>%\n  mutate(submission_category= case_when((Status == \"submitted\" & Modified <= deadline)~ \"On-time\",\n                            (Modified > deadline & deadline <= Extension) ~ \"On-time w extension\",\n                            (Modified > deadline & is.na(Extension)) ~ \"Late\",\n                            (Modified > Extension) ~ \"Late with extension\",\n                            TRUE ~ \"Non-submission\"))\ndat_cleaned %>%\n  count(submission_category) %>%\n  mutate(percent = round((n/total_submissions)*100, 2))\np1 <- dat_cleaned %>%\n  filter(submission_category != \"Non-submission\") %>%\n  ggplot(aes(Modified, fill = submission_category)) +\n  geom_histogram(colour = \"black\") +\n  theme_minimal() + # add theme\n  theme(legend.position = \"bottom\") + # move legend to bottom\n  labs(fill = NULL, x = NULL, title = \"Assignment submission report\") + #labels\n  scale_fill_viridis_d(option = \"E\") + # colour blind friendly palette\n  theme(axis.text.x = element_text(angle=90)) + # rotate axis labels\n  scale_x_datetime(date_breaks = \"1 day\", date_labels = \"%b %d\") + # set breaks\n  geom_vline(xintercept = deadline, colour = \"red\", linetype = \"dashed\", size = 1.5) # add dashed line## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\np1\nggplotly(p1)\ndat_cleaned <- dat_cleaned %>%\n  mutate(submission_category_blunt= case_when((Status == \"submitted\" & \n                                                 Modified <= deadline)~ \"On-time\",\n                            (Modified > deadline) ~ \"Late or extension\",\n                            \n                            TRUE ~ \"Non-submission\"))"},{"path":"moodle-assignment-submission-reports.html","id":"grades","chapter":"3 Moodle Assignment submission reports","heading":"3.4 Grades","text":"use 22-point scale alphanumeric grade corresponding grade point total 22-point scale (e.g., B2 = 16).grades Moodle stored alphanumeric form need convert numbers.often easiest way spreadsheet contains grades associated gradepoints, import , use inner_join() combine two files. join two files common columns, now variable Points dataset corresponding number alphanumeric grade.can now create basic descriptive stats visualisations grade point values. describe() function psych() library great quickly producing range descriptive statistics.stage want highlight simulated data read anything actual patterns, real data, patterns see meaninglessWe also look distribution grades submission category:descriptives category:associated visualization:correlation Turnitin score grade (even real data think bit pointless never know):relationship Turnitin score submission category - code removes two outliers - remove filter line put back .associated visualisation:Finally, also look grades marker:Violin-boxplots:Finally, rather using grouped histograms done previously, better visualise distributions different markers using facet_wrap() makes easier compare distributions:","code":"\ngrade_points <- import(\"https://raw.githubusercontent.com/emilynordmann/scholaRship/master/book/data/grade_points.csv\")\ndat_cleaned <- inner_join(dat_cleaned, grade_points, by = \"Grade\")\ndat_cleaned %>%\n  select(Points) %>% # just select points column for stats\n  describe()\nggplot(dat_cleaned, aes(Points)) +\n  geom_histogram(binwidth = 1, colour = \"black\") +\n  theme_minimal() +\n  labs(title = \"Distribution of essay grades\", \n       x = NULL, \n       colour = NULL, \n       subtitle = \"Dashed line = mean grade\") +\n  scale_x_continuous(breaks = seq(1,22, by = 1)) + \n  geom_vline(aes(xintercept=mean(Points),color=\"red\"), \n             linetype=\"dashed\",\n             size = 1, \n             show.legend = FALSE) \nggplot(dat_cleaned, aes(Points, fill = submission_category)) +\n  geom_histogram(binwidth = 1, colour = \"black\") +\n  theme_minimal() +\n  labs(title = \"Distribution of essay grades\", \n       x = NULL, \n       subtitle = \"Dashed line = mean grade\",\n       fill = NULL) +\n  geom_vline(aes(xintercept=mean(Points),color=\"red\"), \n             linetype=\"dashed\",\n             size=1,\n             show.legend = FALSE) +\n  scale_x_continuous(breaks = seq(1,22, by = 1)) +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d(option = \"E\")\ndat_cleaned %>%\n  group_by(submission_category) %>%\n  summarise(mean_grade = mean(Points, na.rm = TRUE),\n            median_grade = median(Points, na.rm = TRUE))\nggplot(dat_cleaned, aes(x = submission_category, y = Points, fill = submission_category)) +\n  geom_violin(show.legend = FALSE,\n              alpha = .4) +\n  geom_boxplot(width = .2, show.legend = FALSE) +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\") +\n  labs(x = NULL, title = \"Grade point by submission category\")\nggplot(dat_cleaned, aes(Points, Turnitin)) +\n  geom_jitter() + # use jitter rather than geom_point as some overlapping data points\n  geom_smooth(method = \"loess\") + # no clear linear relationship, otherwise use method = \"lm\"\n  labs(x = \"Grade point\", y = \"Turnitin score\")## `geom_smooth()` using formula = 'y ~ x'\ndat_cleaned %>%\n  filter(Turnitin < 75) %>%\n  group_by(submission_category) %>%\n  summarise(mean_grade = mean(Turnitin, na.rm = TRUE),\n            median_grade = median(Turnitin, na.rm = TRUE))\ndat_cleaned %>%\n  filter(Turnitin < 75) %>%\n  ggplot(aes(x = submission_category, y = Turnitin, fill = submission_category)) +\n  geom_violin(show.legend = FALSE,\n              alpha = .4) +\n  geom_boxplot(width = .2, show.legend = FALSE) +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\") +\n  labs(x = NULL, title = \"Turnitin score by submission category\")\ndat_cleaned %>%\n  group_by(Grader) %>%\n  summarise(mean_grade = mean(Points, na.rm = TRUE),\n            median_grade = median(Points, na.rm = TRUE))\ndat_cleaned %>%\n  ggplot(aes(x = Grader, y = Points, fill = Grader)) +\n  geom_violin(show.legend = FALSE,\n              alpha = .4) +\n  geom_boxplot(width = .2, show.legend = FALSE) +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\") +\n  labs(x = NULL, title = \"Grade point by marker\")\nggplot(dat_cleaned, aes(Points, fill = Grader)) +\n  geom_histogram(binwidth = 1, colour = \"black\", show.legend = FALSE) +\n  theme_minimal() +\n  labs(title = \"Distribution of essay grades\", \n       x = \"Grade point\", \n       subtitle = \"Dashed line = mean grade\",\n       fill = NULL) +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d(option = \"E\") +\n  facet_wrap(~Grader, nrow = 4)"},{"path":"workshop.html","id":"workshop","chapter":"4 ALT Conference Prep","heading":"4 ALT Conference Prep","text":"attending workshop ALT conference like live-code along us workshop, please complete set-chapter advance everything need.","code":""},{"path":"workshop.html","id":"installing-r-and-rstudio","chapter":"4 ALT Conference Prep","heading":"4.1 Installing R and RStudio","text":"R RStudio already installed, please follow instructions appendix come back chapter.","code":""},{"path":"workshop.html","id":"workshop-prep","chapter":"4 ALT Conference Prep","heading":"4.2 Workshop prep","text":"worked R basic familiarity code R objects functions work, install packages tidyverse,plotly, scales, ggpubr skip set-check end chapter.R novice, find helpful work workshop.","code":""},{"path":"workshop.html","id":"intro-r-rstudio","chapter":"4 ALT Conference Prep","heading":"4.3 R and RStudio","text":"R programming language write code RStudio Integrated Development Environment (IDE) makes working R easier. Think knowing English using plain text editor like NotePad write book versus using word processor like Microsoft Word. , much harder without things like spell-checking formatting able use advanced features Word developed. similar way, can use R without R Studio recommend . RStudio serves text editor, file manager, spreadsheet viewer, . key thing remember although work using RStudio workshop, actually using two pieces software means time--time, may separate updates.","code":""},{"path":"workshop.html","id":"rstudio_ide","chapter":"4 ALT Conference Prep","heading":"4.3.1 RStudio","text":"installed R, gave computer ability process R programming language, also installed app called \"R\". never use app. Instead, use RStudio. RStudio arranged four window panes.\nFigure 4.1: RStudio IDE\ndefault, upper left pane source pane, view, write, edit code files view data tables spreadsheet format. first open RStudio, pane display open document load data - worry, get soon.lower left pane console pane, can type commands view output messages. can write code console test . code run can create objects environment, code saved. need write code script source pane save .right panes several different tabs show information code. used tabs upper right pane Environment tab Help tab. environment tab lists information objects defined code. learn Help tab Section ??.lower right pane, used tabs Files tab directory structure, Plots tab plots made script, Packages tab managing add-packages (see Section 4.6), Viewer tab display reports created scripts. can change location panes tabs shown Preferences > Pane Layout.","code":""},{"path":"workshop.html","id":"intro-reproducibility","chapter":"4 ALT Conference Prep","heading":"4.3.2 Reproducibility","text":"One main reasons learn R can create reproducible reports. involves writing scripts transform data, create summaries visualisations, embed report way always gives results.things reproducibly, others (future ) can understand check work. can also reuse work easily. example, need create exam board report every semester student grades, reproducible report allows download new data create report within seconds. might take little longer set report first instance reproducible methods, time saves long run invaluable.","code":""},{"path":"workshop.html","id":"themes-and-accessiblilty","chapter":"4 ALT Conference Prep","heading":"4.3.3 Themes and accessiblilty","text":"can customise R Studio looks make work . Click Tools - Global Options - Appearance. can change default font, font size, general appearance R Studio, including using dark mode.","code":""},{"path":"workshop.html","id":"intro-sessions","chapter":"4 ALT Conference Prep","heading":"4.4 Sessions","text":"settings configured correctly, open RStudio start writing code, loading packages, creating objects, new session Environment tab completely empty. find code working figure , might worth restarting R session. clear environment detach loaded packages - think like restarting phone. several ways can restart R:Menu: Session > Restart RCmd-Shift-F10 Ctl-Shift-F10type .rs.restartR() console","code":""},{"path":"workshop.html","id":"functions","chapter":"4 ALT Conference Prep","heading":"4.5 Functions","text":"install R access range functions including options data wrangling statistical analysis. functions included default installation typically referred base R can think like default apps come pre-loaded phone.function name refers code can reuse. using functions provided packages, can also write functions.type function console pane, run soon hit enter. put function script R Markdown document source pane, run run script, knit R Markdown file, run code chunk. learn workshop.example, function sum() included base R, expect. console, run code:","code":"\nsum(1,2,3)## [1] 6"},{"path":"workshop.html","id":"packages","chapter":"4 ALT Conference Prep","heading":"4.6 Packages","text":"One great things R, however, user extensible: anyone can create new add-extends functionality. currently thousands packages R users created solve many different kinds problems, just simply fun. example, packages data visualisation, machine learning, interactive dashboards, web scraping, playing games Sudoku.Add-packages distributed base R, downloaded installed archive, way , instance, download install PokemonGo smartphone. main repository packages reside called CRAN, Comprehensive R Archive Network.important distinction installing package loading package.","code":""},{"path":"workshop.html","id":"install-package","chapter":"4 ALT Conference Prep","heading":"4.6.1 Installing a package","text":"done using install.packages(). like installing app phone: app remain installed remove . instance, want use PokemonGo phone, install App Store Play Store; re-install time want use . launch app, run background close restart phone. Likewise, install package, package available (loaded) every time open R.Install tidyverse package system. package main package use throughout book data wrangling, summaries, visualisation.get message says something like package ‘tidyverse’ successfully unpacked MD5 sums checked, installation successful. get error package installed, check troubleshooting section Appendix .10.Never install package inside script. console pane.can also install multiple packages . command install packages using workshop.","code":"\n# type this in the console pane\ninstall.packages(\"tidyverse\")\npackages <- c(\n  \"tidyverse\",  # for everything\n  \"plotly\",  # Creates interactive plots\n  \"ggpubr\",   # Builds on ggplot2 to build specific publication ready plots\n  \"scales\" # for plot scales\n)\n\ninstall.packages(packages)"},{"path":"workshop.html","id":"loading-a-package","chapter":"4 ALT Conference Prep","heading":"4.6.2 Loading a package","text":"done using library() function. like launching app phone: functionality app launched remains close app restart. example, run library(patchwork) within session, functions package referred plotly made available R session. next time start R, need run library(plotly) want access package.installing thetidyverse package, can load current R session follows:might get red text load package, normal. usually warning package functions name packages already loaded.can use convention package::function() indicate add-package function resides. instance, see readr::read_csv(), refers function read_csv() readr add-package. package loaded using library(), specify package name function unless conflict (e.g., two packages loaded function name).","code":"\nlibrary(tidyverse)"},{"path":"workshop.html","id":"tidyverse","chapter":"4 ALT Conference Prep","heading":"4.6.3 Tidyverse","text":"tidyverseis meta-package loads several packages incredibly useful cleaning, processing, summarising, visualising almost type data:ggplot2, data visualisationreadr, data importtibble, tablestidyr, data tidyingdplyr, data manipulationlubridate dates timesstringr, stringsforcats, factorspurrr, repeating things","code":""},{"path":"workshop.html","id":"using-functions","chapter":"4 ALT Conference Prep","heading":"4.7 Using functions","text":"","code":""},{"path":"workshop.html","id":"arguments","chapter":"4 ALT Conference Prep","heading":"4.7.1 Arguments","text":"functions allow/require specify one morearguments. options can set. can look arguments/options function using help documentation. arguments required, optional. Optional arguments often use default (normally specified help documentation) enter value.example, look help documentation function sample() randomly samples items list.help documentation sample() appear bottom right help panel. usage section, see sample() takes following form:arguments section, explanations arguments. x list items want choose , size number items want choose, replace whether item may selected , prob gives probability item chosen. details section notes values entered replace prob use defaults FALSE (item can chosen ) NULL (items equal probability chosen). default value x size, must specified otherwise code run.try example just change required arguments x size ask R choose 5 random letters (letters built-vector 26 lower-case Latin letters).sample() generates random sample. time run code, generate different set random letters (try ). function set.seed() controls random number generator - using functions use randomness (sample()), running set.seed() ensure get result (many cases may want ). get numbers , run set.seed(1242016) console, run sample(x = letters, size = 5) .Now can change default value replace argument produce set letters allowed duplicates.time R still produced 5 random letters, now set letters two instances \"k\". Always remember use help documentation help understand arguments function requires.","code":"\n?sample\nsample(x, size, replace = FALSE, prob = NULL)\nsample(x = letters, size = 5)## [1] \"z\" \"v\" \"y\" \"w\" \"j\"\nset.seed(8675309)\nsample(x = letters, size = 5, replace = TRUE)## [1] \"t\" \"k\" \"j\" \"k\" \"m\""},{"path":"workshop.html","id":"argument-names","chapter":"4 ALT Conference Prep","heading":"4.7.2 Argument names","text":"examples, written argument names code (.e., x, size, replace), however, strictly necessary. following two lines code produce result (although time run sample() produce slightly different result, random, still work ):Importantly, write argument names, R use default order arguments. sample assume first value enter x. second value size third value replace.write argument names can write arguments whatever order like:first learning R, may find useful write argument names can help remember understand part function . However, skills progress may find quicker omit argument names also see examples code online use argument names, important able understand argument bit code referring (look help documentation check).workshop, always write argument names first time use function. However, subsequent uses may omitted.","code":"\nsample(x = letters, size = 5, replace = TRUE)\nsample(letters, 5, TRUE)\nsample(size = 5, replace = TRUE, x = letters)"},{"path":"workshop.html","id":"tab-auto-complete","chapter":"4 ALT Conference Prep","heading":"4.7.3 Tab auto-complete","text":"One useful feature R Studio tab auto-complete functions (see Figure 4.2). write name function press tab key, R Studio show arguments function takes along brief description. press enter argument name fill name , just like auto-complete phone. incredibly useful first learning R remember use feature frequently.\nFigure 4.2: Tab auto-complete\n","code":""},{"path":"workshop.html","id":"objects","chapter":"4 ALT Conference Prep","heading":"4.8 Objects","text":"large part coding involve creating manipulating objects. Objects contain stuff. stuff can numbers, words, result operations analyses. assign content object using <-.Run following code console, change values name age details change christmas holiday date care .see four objects now appear environment pane:name character (text) data. order R recognise character data, must enclosed double quotation marks \" \".age numeric data. order R recognise number, must enclosed quotation marks.today stores result function Sys.Date(). function returns computer system's date. Unlike name age, hard-coded (.e., always return values enter), contents object today change dynamically date. , run function tomorrow, update date tomorrow's date.christmas also date hard-coded specific date. wrapped within .Date() function tells R interpret character string provide date rather text.print contents object, type object's name console press enter. Try printing four objects now.Finally, key concept understand objects can interact can save results interactions new object. Edit run following code create new objects, print contents new object.","code":"\nname <- \"Emily\"\nage <- 38\ntoday <- Sys.Date()\nchristmas <- as.Date(\"2023-12-25\")\ndecade <- age + 10\nfull_name <- paste(name, \"Nordmann\")\nhow_long <- christmas - today"},{"path":"workshop.html","id":"help","chapter":"4 ALT Conference Prep","heading":"4.9 Getting help","text":"feel like need lot help starting learn. really go away; impossible memorise everything. goal learn enough structure R can look things quickly. introduce specialised jargon glossary; easier google \"convert character numeric R\" \"make numbers quotes actual numbers words\". addition function help described , additional resources use often.","code":""},{"path":"workshop.html","id":"package-reference-manuals","chapter":"4 ALT Conference Prep","heading":"4.9.1 Package reference manuals","text":"Start help browser entering help.start() console. Click \"Packages\" \"Reference\" see list packages. Scroll readxl package click see list functions available package.","code":""},{"path":"workshop.html","id":"googling","chapter":"4 ALT Conference Prep","heading":"4.9.2 Googling","text":"function help help, even sure function need, try Googling question. take practice able use right jargon search terms get want. helps put \"R\" \"tidyverse\" search text, name relevant package, like ggplot2.","code":""},{"path":"workshop.html","id":"vignettes","chapter":"4 ALT Conference Prep","heading":"4.9.3 Vignettes","text":"Many packages, especially tidyverse ones, helpful websites vignettes explaining use functions. vignettes also available inside R. can access package's help page vignette() function.","code":"\n# opens a list of available vignettes\nvignette(package = \"ggplot2\")\n\n# opens a specific vignette in the Help pane\nvignette(\"ggplot2-specs\", package = \"ggplot2\")"},{"path":"workshop.html","id":"setup-check","chapter":"4 ALT Conference Prep","heading":"4.10 Workshop set-up check","text":"Restart R session run code copying pasting console hitting enter. managed install update software packages required, run without issue produce histograms. produce messages look like errors involving stat_bin non-finite values, worry, errors explain messages mean workshop.get error package called..., make sure installed packages listed Section 4.6.1.technical issues working machine get code run, please use RStudio Cloud workshop time troubleshoot installation problems.","code":"\nlibrary(tidyverse)\nlibrary(ggpubr)\n\n# Using ggplot2 to visualize distribution of height\ngg_height <- ggplot(starwars, aes(x = height)) + \n  geom_histogram(binwidth = 10, fill = \"blue\", alpha = 0.7) + \n  labs(title = \"Distribution of Heights of Star Wars Characters\", x = \"Height (cm)\", y = \"Count\")\n\n# Using ggplot2 to visualize distribution of mass\ngg_mass <- ggplot(starwars, aes(x = mass)) + \n  geom_histogram(binwidth = 10, fill = \"red\", alpha = 0.7) + \n  labs(title = \"Distribution of Mass of Star Wars Characters\", x = \"Mass (kg)\", y = \"Count\")\n\n# Displaying the plots side-by-side using ggpubr\nggarrange(gg_height, gg_mass, ncol = 2, common.legend = TRUE)"},{"path":"workshop.html","id":"resources-intro","chapter":"4 ALT Conference Prep","heading":"4.11 Further Resources","text":"RStudio IDE CheatsheetRStudio Cloud","code":""},{"path":"Echo_tidy.html","id":"Echo_tidy","chapter":"5 Getting started with Echo360 data with tidyverse","heading":"5 Getting started with Echo360 data with tidyverse","text":"","code":""},{"path":"Echo_tidy.html","id":"reading-data-into-r","chapter":"5 Getting started with Echo360 data with tidyverse","heading":"5.1 Reading data into R","text":"start tutorial, install following packages. note comment need package .readr package tidyverse provides us host functions reading data R. Often course, multiple video recordings Echo360 data . useful keep data within one data frame R analysis, often consider metrics across full course one video.list.files command list files available within given working directory. download data tutorial, please download .zip file containing 9 files Echo360 data.set working directory , create folder named data computer extract zip folder data (folder named data folder named Echo360_data ).See Chapter 4 created anonymous synthetic data use tutorials. data stored .csv files can specifically list files using command pattern = \".csv\" shown :object created (data) contains names various Echo360 videos variable video, though naming conventions awkward handle R. can simply number videos using code .","code":"\nlibrary(tidyverse) # Package of packages for plotting and wrangling \nlibrary(psych) # for descriptive statistics\n# Obtain list of files from directory\nfiles <- list.files(path = \"data/Echo360_Data/\", \n                    pattern=\".csv\") \n\n# Read in all files\ndata <- read_csv(paste0(\"data/Echo360_Data/\", files), # Add our working directory to the list of files \n                 id=\"video\") # What should be call the column containing the name of the video file? \n\n# Fill in spaces between column names\ndata <- tibble(data, \n               .name_repair = \"universal\") # Setting to universal makes all names unique and syntactic\n# To break this code down, we start with the innermost function\n# 1. We first make each unique video name a factor (unique category)\n# 2. We then make each factor a number, so we get an ascending number from 1 to 9\n# 3. We then make the numbers a factor, because they're actually labels rather than real numbers\ndata$video <-as.factor(as.numeric(as.factor(data$video)))"},{"path":"Echo_tidy.html","id":"data-descriptions-for-each-field-in-downloaded-data","chapter":"5 Getting started with Echo360 data with tidyverse","heading":"5.2 Data descriptions for each field in downloaded data","text":"Within Echo360 data, various fields data different formats. important understand fields corresponds exploratory analysis data.R often read data correct format store variables right way. However, always case, best check carried correctly. can obtain summary column data using str command:output command details variable type column stored R. example, owner_name character variable, whereas total_views numerical.alternative way check data use function glimpse() gives similar summary different output format:Whichever method use, point, worthwhile checking raw data (visually inspecting .csv file(s) read ) output str() check R correctly converted column preferred variable type.","code":"\nstr(data)## tibble [1,466 × 16] (S3: tbl_df/tbl/data.frame)\n##  $ video            : Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ media_id         : chr [1:1466] \"631c9eb6-7828-4e41-9e9e-ae3b261ae741\" \"631c9eb6-7828-4e41-9e9e-ae3b261ae741\" \"631c9eb6-7828-4e41-9e9e-ae3b261ae741\" \"631c9eb6-7828-4e41-9e9e-ae3b261ae741\" ...\n##  $ media_name       : chr [1:1466] \"Physiological Psychology Week 1 Part 1\" \"Physiological Psychology Week 1 Part 1\" \"Physiological Psychology Week 1 Part 1\" \"Physiological Psychology Week 1 Part 1\" ...\n##  $ create_date      : chr [1:1466] \"01/07/2022\" \"01/07/2022\" \"01/07/2022\" \"01/07/2022\" ...\n##  $ duration         : 'hms' num [1:1466] 00:13:11 00:13:11 00:13:11 00:13:11 ...\n##   ..- attr(*, \"units\")= chr \"secs\"\n##  $ owner_name       : chr [1:1466] \"James Bartlett\" \"James Bartlett\" \"James Bartlett\" \"James Bartlett\" ...\n##  $ course           : chr [1:1466] \"Physiological Psychology (PSYCH4065/5029) 2022-23\" \"Physiological Psychology (PSYCH4065/5029) 2022-23\" \"Physiological Psychology (PSYCH4065/5029) 2022-23\" \"Physiological Psychology (PSYCH4065/5029) 2022-23\" ...\n##  $ user_name        : chr [1:1466] \"Ianto, Tasadduq\" \"Alessa, Tamarah\" \"Mary-Ann, Abdussalam\" \"Aderyn, Cahil\" ...\n##  $ email_address    : chr [1:1466] \"210396@university.ac.uk\" \"205650@university.ac.uk\" \"211437@university.ac.uk\" \"298179@university.ac.uk\" ...\n##  $ total_views      : num [1:1466] 1 1 1 2 2 4 2 2 1 1 ...\n##  $ total_view_time  : 'hms' num [1:1466] 00:10:55 00:12:46 00:13:09 00:10:32 ...\n##   ..- attr(*, \"units\")= chr \"secs\"\n##  $ average_view_time: 'hms' num [1:1466] 00:10:55 00:12:46 00:13:09 00:05:16 ...\n##   ..- attr(*, \"units\")= chr \"secs\"\n##  $ on_demand_views  : num [1:1466] 1 1 1 2 2 4 2 2 1 1 ...\n##  $ live_view_count  : num [1:1466] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ downloads        : num [1:1466] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ last_viewed      : chr [1:1466] \"01/16/2023\" \"01/18/2023\" \"01/24/2023\" \"01/15/2023\" ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   media_id = col_character(),\n##   ..   media_name = col_character(),\n##   ..   create_date = col_character(),\n##   ..   duration = col_time(format = \"\"),\n##   ..   owner_name = col_character(),\n##   ..   course = col_character(),\n##   ..   user_name = col_character(),\n##   ..   email_address = col_character(),\n##   ..   total_views = col_double(),\n##   ..   total_view_time = col_time(format = \"\"),\n##   ..   average_view_time = col_time(format = \"\"),\n##   ..   on_demand_views = col_double(),\n##   ..   live_view_count = col_double(),\n##   ..   downloads = col_double(),\n##   ..   last_viewed = col_character()\n##   .. )\n##  - attr(*, \"problems\")=<externalptr>\nglimpse(data)## Rows: 1,466\n## Columns: 16\n## $ video             <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ media_id          <chr> \"631c9eb6-7828-4e41-9e9e-ae3b261ae741\", \"631c9eb6-78…\n## $ media_name        <chr> \"Physiological Psychology Week 1 Part 1\", \"Physiolog…\n## $ create_date       <chr> \"01/07/2022\", \"01/07/2022\", \"01/07/2022\", \"01/07/202…\n## $ duration          <time> 00:13:11, 00:13:11, 00:13:11, 00:13:11, 00:13:11, 0…\n## $ owner_name        <chr> \"James Bartlett\", \"James Bartlett\", \"James Bartlett\"…\n## $ course            <chr> \"Physiological Psychology (PSYCH4065/5029) 2022-23\",…\n## $ user_name         <chr> \"Ianto, Tasadduq\", \"Alessa, Tamarah\", \"Mary-Ann, Abd…\n## $ email_address     <chr> \"210396@university.ac.uk\", \"205650@university.ac.uk\"…\n## $ total_views       <dbl> 1, 1, 1, 2, 2, 4, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2…\n## $ total_view_time   <time> 00:10:55, 00:12:46, 00:13:09, 00:10:32, 00:19:58, 0…\n## $ average_view_time <time> 00:10:55, 00:12:46, 00:13:09, 00:05:16, 00:09:59, 0…\n## $ on_demand_views   <dbl> 1, 1, 1, 2, 2, 4, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2…\n## $ live_view_count   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ downloads         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ last_viewed       <chr> \"01/16/2023\", \"01/18/2023\", \"01/24/2023\", \"01/15/202…"},{"path":"Echo_tidy.html","id":"quick-summaries-of-data","chapter":"5 Getting started with Echo360 data with tidyverse","heading":"5.3 Quick summaries of data","text":"can obtain quick summaries variables get early feel data check potential errors outliers data.","code":""},{"path":"Echo_tidy.html","id":"numerical-data","chapter":"5 Getting started with Echo360 data with tidyverse","heading":"5.3.1 Numerical data","text":"numerical data, can obtain series numerical summaries.select() command allows us select variables based specific criteria (variable name condition). , select variables numeric using (.numeric). can obtain mean numeric variables using summarise_all() command specifying chosen summary metric (, mean).can compute mean observations numerical columns using following code:summary stats suggest distribution data might normal - mean live view count downloads zero. see going , can visualise spread data histograms.plots look bit weird - turns values variables zero.total views demand views, mean value identical can see plotting data indeed data - makes sense 0 live views must come demand views demand viewing figures equal total viewing figures. Knowing helps us better understand utility variable.Given distribution, case mean useful also need variables can just select ones useful compute range stats using describe() function psych package. case, eithr select total_views on_demand_views thing. go total_views shorter variable name.order describe() work, need transform object data frame currently stored tibble (type data object used tidyverse).Another way represent count data use summarise() function n(). n() can unintuitive need pass arguments , simply count whatever given . case, pass data use group_by() tell group data value total_views. means n() count many observations total viewing number.group_by() function allows us carry computations groups. can use summarise() obtain total number counts total number views using n().Although code chapter really extend can get Echo360 dashboards, loading checking data simple descriptive stats visualisation can help identify issues help better understand data fr complex analyses.","code":"\ndata %>% # The data frame you are using\n  # select chooses or omits the columns you want included\n  select(where(is.numeric)) %>% # By using where() within select(), we can ask R to show us all the numeric variables\n  summarise_all(mean) # For all the numeric variables, calculate the mean value\nggplot(data, aes(x = live_view_count)) +\n  geom_histogram()\n\nggplot(data, aes(x = downloads)) +\n  geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\ndata %>%\n  count(live_view_count)\n\ndata %>%\n  count(downloads)\nggplot(data, aes(x = total_views)) +\n  geom_histogram()\n\nggplot(data, aes(x = on_demand_views)) +\n  geom_histogram()\ndata %>%\n  select(total_views) %>%\n  as.data.frame() %>%\n  describe()\ndata %>% # The data frame you are using\n  group_by(total_views) %>% # Group by the total_views variable\n  summarise(n = n()) # Calculate the number of observations for each group"},{"path":"Echo_course.html","id":"Echo_course","chapter":"6 Exploring Echo360 video/course level data with tidyverse","heading":"6 Exploring Echo360 video/course level data with tidyverse","text":"","code":""},{"path":"Echo_course.html","id":"introduction-1","chapter":"6 Exploring Echo360 video/course level data with tidyverse","heading":"6.1 Introduction","text":"Now introduced basic principles reading summarising Echo360 data, chapter explore can wrangle visualise video course level data recreate build kind dashboards can access Echo360.need following packages chapter 6, addition scales add useful functions plotting graph axes. also need data object created chapter 6, can rerun code available already. data downloaded already, please download .zip file containing 9 files Echo360 data. tutorial assumes folder called \"data\" working directory, created subfolder called \"Echo360_Data\" place files. saved data another way, remember edit file paths first.","code":"\nlibrary(tidyverse) # Package of packages for plotting and wrangling \nlibrary(plotly) # Creates interactive plots \nlibrary(ggpubr) # Builds on ggplot2 to build specific publication ready plots \nlibrary(scales) # Includes functions for specifying plot scales\nlibrary(lubridate)\n# Obtain list of files from directory\nfiles <- list.files(path = \"data/Echo360_Data/\", \n                    pattern=\".csv\") \n\n# Read in all files\ndata <- read_csv(paste0(\"data/Echo360_Data/\", files), # Add our working directory to the list of files \n                 id=\"video\") # What should be call the column containing the name of the video file? \n\n# Fill in spaces between column names\ndata <- tibble(data, \n               .name_repair = \"universal\") # Setting to universal makes all names unique and syntactic\n\n# To break this code down, we start with the innermost function\n# 1. We first make each unique video name a factor (unique category)\n# 2. We then make each factor a number, so we get an ascending number from 1 to 9\ndata$video <-as.factor(as.numeric(as.factor(data$video)))"},{"path":"Echo_course.html","id":"handling-datetime-data-with-lubridate","chapter":"6 Exploring Echo360 video/course level data with tidyverse","heading":"6.2 Handling date/time data with lubridate","text":"important able track engagment lecture recordings throughout semester able analyse data according time data importnat. Handling date/time data R somewhat different variable types treated differently handled care. lubridate package R (loaded part tidyverse) allows us easily handle tricky date/time data extract useful information .","code":""},{"path":"Echo_course.html","id":"converting-variables-to-datetime-format","chapter":"6 Exploring Echo360 video/course level data with tidyverse","heading":"6.2.1 Converting variables to date/time format","text":"first task convert date/time data correct format. sample data working , five variables contain data : Create.Date, Duration, Total.View.Time, Average.View.Time Last.Viewed.closer inspection, variables fall two types:Create.Data Last.Viewed listed date (particular day)Create.Data Last.Viewed listed date (particular day)Duration, Total.View.Time Average.View.Time listed time, recorded hours, minutes, seconds.Duration, Total.View.Time Average.View.Time listed time, recorded hours, minutes, seconds.Several functions can used take data string convert desired date/time format. useful cheat sheet can download lubridate library contains examples functions. data, use mdy() function convert dates month-day-year, hms() function convert times hours-minutes-seconds., use combination mutate across apply given function multiple columns. Within across, specify two arguments. .cols, specify columns want apply function .fns, specify function want apply columns.hms() converts string date/time object set hours-minutes-seconds. mdy() converts string date/time object set month-day-year. Echo360 data saves date/times US format, pay attention date/time data stored data work make sure recognises information right order. alternative functions like ymd() dmy() data work stored differently.can take look result transformations closely using head() preview first five cases variable:","code":"\n# For times, mutate across three columns and convert to hours, minutes, seconds\n# For dates, mutate across two columns and convert to month, day, year\ndata <- data %>%\n  mutate(across(.cols = c('duration', 'total_view_time', 'average_view_time'), \n                .fns = hms)) %>%\n  mutate(across(.cols = c('create_date', 'last_viewed'), \n                .fns = mdy))\n# First five cases of duration \nhead(data$duration)\n# First five cases of last viewed\nhead(data$last_viewed)## [1] \"13M 11S\" \"13M 11S\" \"13M 11S\" \"13M 11S\" \"13M 11S\" \"13M 11S\"\n## [1] \"2023-01-16\" \"2023-01-18\" \"2023-01-24\" \"2023-01-15\" \"2023-01-16\"\n## [6] \"2023-02-15\""},{"path":"Echo_course.html","id":"transforming-time-data","chapter":"6 Exploring Echo360 video/course level data with tidyverse","heading":"6.2.2 Transforming time data","text":"help us summarise visualise time data, can convert time variables numeric. case, can use mutate() .numeric() convert variable duration time variable (e.g., 13M 11S) number (13.2). easily allows us compute summary statistics visualise data","code":"\n# compute mean duration\ndata %>%\n  mutate(duration = as.numeric(duration, \"minutes\")) %>%\n  group_by(video) %>%\n  summarise(duration = mean(duration))\n\n# produce bar chart\ndata %>%\n  mutate(duration = as.numeric(duration, \"minutes\")) %>%\n  group_by(video) %>%\n  summarise(duration = mean(duration)) %>%\n  ggplot(aes(x = video, y = duration, fill = video)) + # fill gives colours for each video\n  geom_col() +\n  guides(fill = \"none\") + # removes redundant legend\n  theme_minimal() + # apply a theme\n  labs(title = \"Video length in minutes\")\n# produce line graph of same data\ndata %>%\n  mutate(duration = as.numeric(duration, \"minutes\")) %>%\n  group_by(video) %>%\n  summarise(duration = mean(duration)) %>%\n  ggplot(aes(x = video, y = duration, colour = video, group = 1)) + # fill gives colours for each video\n  geom_point() +\n  geom_line() +\n  guides(colour = \"none\") + # removes redundant legend\n  theme_minimal() + # apply a theme\n  labs(title = \"Video length in minutes\") +\n  scale_y_continuous(limits = c(0, 20)) #  set limits of y-axis"},{"path":"Echo_course.html","id":"extracting-elements-from-datetime-data","chapter":"6 Exploring Echo360 video/course level data with tidyverse","heading":"6.2.3 Extracting elements from date/time data","text":"Sometimes, may wish obtain specific parts date/time month, minutes etc. several functions lubridate allow us extract easily date/time object.Suppose want look last month students viewed videos across course. can obtain applying month() function Last.Viewed variable.can also convert bar chart make interactive using ggplotly() function. allows hover plot see frequency month.","code":"\n# Add a new month column \ndata <- data %>% \n  mutate(month_last_viewed = month(data$last_viewed))\n\nlast_viewed <- data %>% \n  ggplot(aes(x = month_last_viewed)) + # Only specify x axis for a bar chart\n  geom_bar() + # Create a bar chart \n  labs(title = \"Frequency of last month video viewed\",\n       y = \"Frequency\",\n       x = \"Last month video viewed\") + \n  theme_bw() + \n  scale_x_continuous(breaks = seq(1:12))\n\nlast_viewed\nggplotly(last_viewed)"},{"path":"Echo_course.html","id":"maths-with-date-times","chapter":"6 Exploring Echo360 video/course level data with tidyverse","heading":"6.2.4 Maths with date-times","text":"Sometimes, may wish compare difference time events. lubridate provides useful functions help us . example, can look difference view time total duration video using simple arithmetic operators combined group_by(). noted, function group_by() perform whatever operation comes seperately level grouping variable case, compute time difference video.data returns values minutes seconds, can transform numeric create histogram. negative values represent students watched less video total duration., can make values interactive converting ggplot visualisation plotly object.difference viewing time duration course relative duration may better express difference percent, .e., average, percent video students watch?, can use mutate calculate percent video viewed. created column, can use group_by() ggplot() summarise visualise drop-video.","code":"\ndata <- data %>% \n  group_by(video) %>%\n  mutate(time_difference = average_view_time - duration) %>% # Add a time difference column between average view time and total duration\n  ungroup()\n\nhead(data$time_difference)## [1] \"-3M 44S\"  \"-1M 35S\"  \"-2S\"      \"-8M 5S\"   \"-4M 48S\"  \"-11M -9S\"\ndata <- data %>% \n  mutate(time_difference = as.numeric(time_difference, \"minute\"))\n\ntime_difference <- data %>% \n  ggplot(aes(x = time_difference)) + \n  geom_histogram() +\n  labs(title=\"Difference between average viewing times & total video duration\") +\n  theme_bw() \n\ntime_difference## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplotly(time_difference)## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\ndata <- data %>%\n  mutate(percent_watched = 100 - ((duration-average_view_time) / duration) *100) \n\ndata %>%\n  group_by(video) %>%\n  summarise(mean_percent = mean(percent_watched))\n\n\ndata %>%\n  group_by(video) %>%\n  summarise(mean_percent = mean(percent_watched)) %>%\n  ggplot(aes(x = video, y = mean_percent, colour = video, group = 1)) + # fill gives colours for each video\n  geom_point() +\n  geom_line() +\n  guides(colour = \"none\") + \n  theme_minimal() + \n  labs(title = \"Video length in minutes\") +\n  scale_y_continuous(limits = c(0, 100)) #  set limits of y-axis"},{"path":"Echo_course.html","id":"total-views-for-each-video","chapter":"6 Exploring Echo360 video/course level data with tidyverse","heading":"6.3 Total views for each video","text":"following section, continuing creating plots data summarised video level. Given data currently stored student level (one row per student per video), first transform data new data set called video_data group data video create variables interest. First, create new data set two columns:video: video number ;video: video number ;total_views: total number views per video.total_views: total number views per video.can now easily create bar chart total number views per video., plot video number along x-axis total number views video y-axis. numbers include duplications students watched videos multiple times.may also interested total number unique views, .e. total number students watched video least . , need create new variable called unique_views contain sum rows video. can add video-level data set video_data.can edit code previous bar chart create bar chart total number unique student views per video follows.want view values per video precisely like data dashboard, can convert unique views bar plot plotly object.","code":"\nvideo_data <- data %>% \n  group_by(video) %>%\n  summarise(total_views = sum(total_views)) %>% # for each video, add up all the views across students\n  ungroup() # Retaining the group by can sometimes cause problems\nfig.total.views <- video_data %>% \n  ggplot(aes(x = video, y = total_views)) +\n  geom_col() + \n  labs(title=\"Total number of views per video\",\n       y = \"Total Views\",\n       x = \"Video\") + \n  theme_bw()\n\nfig.total.views\nvideo_data <- data %>% \n  group_by(video) %>%\n  summarise(total_Views = sum(total_views), \n            unique_views = n()) %>% # For each video, count the number of rows\n  mutate(video = factor(video)) %>% \n  ungroup()\nfig.unique.views <- video_data %>% \n  ggplot(aes(x = video, y = unique_views)) +\n  geom_bar(stat = \"identity\") + # Since we specify a y value, just show as the value\n  labs(title=\"Total number of unique student views per video\",\n       y = \"Unique Student Views\",\n       x = \"Video\") + \n  theme_bw()\n\nfig.unique.views\nggplotly(fig.unique.views)"},{"path":"Echo_course.html","id":"advanced-plotly-customisation","chapter":"6 Exploring Echo360 video/course level data with tidyverse","heading":"6.3.1 Advanced plotly customisation","text":"far, simply transformed ggplots plotly object. However, can add additional information hover text total views, duration video, average view time might useful viewer.going longer process, break steps. First, need wrangle data key summary statistics per video. first lines follow process , add additional summary variables like average view time modify existing variables mutate().video, now bunch summary statistics like view time, duration, percentage video viewed. can now use information present dashboard overview video.time, going use plotly directly easier edit hover text options simply convert ggplot object plotly. code , first create bar plot number unique views video. edit hover template add additional information interactive dashboard like duration average view time.Like ggplot2, defined plotly object, can add additional layers. example, might also want create stacked bar plot total number views split unique student views repeated views.","code":"\n# Isolate video number and duration of the video to append later\nvideo_duration <- data %>% \n  distinct(video, duration) %>% \n  mutate(duration = period_to_seconds(duration)) # Convert duration from a period to seconds\n\nvideo_data <- data %>%\n  group_by(video) %>%\n  summarise(total_views = sum(total_views), \n            unique_views = n(), \n            average_view_time = round(\n              mean(period_to_seconds(average_view_time), # lubridate function to transform a period to seconds\n                   na.rm = T), # Ignore NA values when taking mean\n              0)) %>% # 0 decimals to round\n  left_join(video_duration, # Add video duration data to our summary data\n            by = \"video\") %>% # Join by video number\n  mutate(video = factor(video),\n         repeated_views = total_views - unique_views, # Calculate repeated views subtracting unique views from total views\n         percentage_viewed = round(average_view_time / duration * 100, 2)) %>% # Calculate average percentage from average view time as percentage of duration\n  ungroup()\n\nhead(video_data)\nfig.all.views <- plot_ly(video_data,\n  x = ~video,\n  y = ~unique_views,\n  name = \"Unique Views\",\n  type = \"bar\",\n  hovertemplate = paste0(\"Video %{x}\", # Paste adds all the elements together, <br> is html code for a line break\n                         \"<br>Total views:\", video_data$total_views,\n                         \"<br>Unique views: %{y}\",\n                         \"<br> Duration: \", seconds_to_period(video_data$duration), # Convert from seconds to minutes and seconds for easier viewing\n                         \"<br> Average view time: \", seconds_to_period(video_data$average_view_time))) %>%\n  layout(title = 'Unique Views per Video', \n         xaxis = list(title = 'Video Number'),\n         yaxis = list(title = 'Unique Views'))\n\nfig.all.views\n# Take the plotly object and add a trace (variable to split by) for the repeated views\nfig.all.views <- fig.all.views %>% \n  add_trace(y = ~repeated_views, \n            name = 'Repeated views') %>% \n  layout(yaxis = list(title = 'Count'), # Amend the type of bar plot to make it stacked\n         barmode = 'stack')\n\nfig.all.views"},{"path":"Echo_course.html","id":"video-duration-vs.-average-percentage-viewed","chapter":"6 Exploring Echo360 video/course level data with tidyverse","heading":"6.4 Video duration vs. average percentage viewed","text":"one final type data visualisation, create scatterplot video duration average percentage video viewed using \"ggplot()\"., can convert scatterplot plotly object make interactive scroll across values.","code":"\nfig.duration.viewed <- video_data %>% \n  ggplot(aes(x = duration, y = percentage_viewed)) +\n  geom_point() + \n  labs(title = \"Video duration vs. average percentage viewed\",\n       y = \"Average percentage of video viewed\",\n       x = \"Video duration (Seconds)\") + \n  theme_bw() + \n  scale_y_continuous(breaks = pretty_breaks()) # Convert the y axis to tidier labels for percentages\n\nfig.duration.viewed\nggplotly(fig.duration.viewed)"},{"path":"Echo_course.html","id":"advanced-plotly-customisation-1","chapter":"6 Exploring Echo360 video/course level data with tidyverse","heading":"6.4.1 Advanced plotly customisation","text":"Like demonstrated bar plot, can create scatterplot entirely within plotly allow us modify hover template include additional summary statistics.","code":"\nfig.duration.viewed.2 <- plot_ly(video_data,\n               x = ~duration,\n               y = ~percentage_viewed,\n               type = \"scatter\",\n               mode = \"markers\",\n               hovertemplate = paste0(\"Video \", video_data$video,\n                                      \"<br>Average percentage viewed: \", round(video_data$percentage_viewed,1), \"%\",\n                                      \"<br> Duration: \", video_data$duration,\n                                      \"<br> Average view time: \", video_data$average_view_time,\n                                      \"<extra><\/extra>\"))%>%\n  layout(xaxis = list(title = 'Duration of video'),\n         yaxis = list(title = 'Percentage of video viewed'))\n\nfig.duration.viewed.2"},{"path":"Echo_combine.html","id":"Echo_combine","chapter":"7 Combining Echo360 data with other sources of data with tidyverse","heading":"7 Combining Echo360 data with other sources of data with tidyverse","text":"","code":""},{"path":"Echo_combine.html","id":"introduction-2","chapter":"7 Combining Echo360 data with other sources of data with tidyverse","heading":"7.1 Introduction","text":"far, focused analysing Echo360 data isolation. However, might interested combining data multiple sources explore relationships lecture capture engagement/attainment data.tutorial, demonstrate can combine data multiple sources kind identifiers need match people across data sets. addition Echo360 data used far, also two types data virtual learning environment (VLE) Moodle, based introductory research methods course (RM1). mock checklist data many tasks people complete week mock attainment data scores multiple choice quiz (MCQ). applied synthetic data process created anonymous names emails provide consistent information across files join.follow tutorial, please download Echo360 Moodle data .zip file..","code":"\nlibrary(tidyverse) # Package of packages for plotting and wrangling \nlibrary(plotly) # Creates interactive plots ## \n## Attaching package: 'plotly'## The following object is masked from 'package:ggplot2':\n## \n##     last_plot## The following object is masked from 'package:stats':\n## \n##     filter## The following object is masked from 'package:graphics':\n## \n##     layout\nlibrary(ggpubr) # Builds on ggplot2 to build specific publication ready plots "},{"path":"Echo_combine.html","id":"reading-in-the-data","chapter":"7 Combining Echo360 data with other sources of data with tidyverse","heading":"7.2 Reading in the data","text":"first step read data need tutorial. first example, read data one lecture. later examples, combine data multiple lectures. three lectures Echo360 data course, reduce repetition scale better working files, still read data listing files instead assigning file individually object. using relatives paths tutorial assume folder called data working directory two subfolders called RM1_duplicate Moodle_duplicate. saved data another way, need edit paths.individual files checklist MCQ data, can assign individual objects.","code":"\n# One lecture for an example \nlecture_01 <- read_csv(\"data/RM1_duplicate/Lecture01.csv\")\n\n# Reading the Echo360 data from three lectures into one data object\n# Obtain list of files from directory\nfiles <- list.files(path=\"data/RM1_duplicate/\", \n                    pattern=\".csv\", \n                    full.names = TRUE) \n\n# Read in all the files to one object and label each file name\nEcho360_data <- read_csv(files,\n                         id=\"file_name\")\n\n# Preview the columns and data\nglimpse(Echo360_data)## Rows: 145\n## Columns: 16\n## $ file_name         <chr> \"data/RM1_duplicate/Lecture01.csv\", \"data/RM1_duplic…\n## $ media_id          <chr> \"28dcfb20-a198-438b-8dbe-1b5725514231\", \"28dcfb20-a1…\n## $ media_name        <chr> \"RM1 Lecture 1\", \"RM1 Lecture 1\", \"RM1 Lecture 1\", \"…\n## $ create_date       <chr> \"09/23/2022\", \"09/23/2022\", \"09/23/2022\", \"09/23/202…\n## $ duration          <time> 01:00:00, 01:00:00, 01:00:00, 01:00:00, 01:00:00, 0…\n## $ owner_name        <chr> \"James Bartlett\", \"James Bartlett\", \"James Bartlett\"…\n## $ course            <chr> \"Research Methods 1 (PGT Conv)\", \"Research Methods 1…\n## $ user_name         <chr> \"Yaw, Collin\", \"Viaan, Jordan\", NA, NA, \"Sunny, Hali…\n## $ email_address     <chr> \"214488@university.ac.uk\", \"211717@university.ac.uk\"…\n## $ total_views       <dbl> 1, 1, 1, 3, 1, 1, 1, 2, 2, 2, 1, 2, 1, 3, 3, 1, 1, 5…\n## $ total_view_time   <time> 00:00:30, 00:01:00, 00:00:08, 00:04:30, 00:01:00, 0…\n## $ average_view_time <time> 00:00:30, 00:01:00, 00:00:08, 00:01:30, 00:01:00, 0…\n## $ on_demand_views   <dbl> 1, 1, 1, 3, 1, 1, 1, 2, 2, 2, 1, 2, 1, 3, 3, 1, 1, 5…\n## $ live_view_count   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ downloads         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ last_viewed       <chr> \"11/04/2022\", \"09/30/2022\", \"02/10/2023\", \"10/21/202…\n# Reading student data from other Moodle activities into two data objects\nChecklist <- read.csv(\"data/Moodle_duplicate/Checklist.csv\")  #A log of checklist elements clicked\n\nMCQ <- read.csv(\"data/Moodle_duplicate/MCQ.csv\")  #Results from Multiple Choice Quiz"},{"path":"Echo_combine.html","id":"joining-data-using-dplyr","chapter":"7 Combining Echo360 data with other sources of data with tidyverse","heading":"7.3 Joining data using dplyr","text":"dplyr package tidyverse several functions merge \"join\" data. several versions functions depending want join files. detailed overview, please refer dplyr reference page online.two main types joins add match columns two data frames. inner_join() retains observations data frame 1 also matching case data frame 2. different types outer joins retain observations appear least one data frame, left_join() retains observations data frame 1.illustrate functions, consider data Echo360 together data checklist records percentage tasks student marked complete also data containing results multiple choice quiz. checklist MCQ data objects, student appears .","code":""},{"path":"Echo_combine.html","id":"merging-data-where-each-student-only-appears-once-in-each-data-object","chapter":"7 Combining Echo360 data with other sources of data with tidyverse","heading":"7.3.1 Merging data where each student only appears once in each data object","text":"interested combining Echo360 data just one lecture (thus student appear ) together MCQ data follow following steps.First, select variables interest data object, lecture_01 contains Echo360 data first lecture.Second, specify type 'join' apply. tutorial, demonstrate different types join show resulting objects differ. Echo360 Moodle store data differently, need identifier use key link people files. participants' email consistent identifier, use link participants across files. identifying information like name student ID number consistent across files.","code":"\n# Manually select which columns to retain\nlecture_01_merge <- lecture_01 %>% \n  select(user_name,\n         email_address,\n         total_views,\n         total_view_time,\n         average_view_time,\n         last_viewed)\n\nMCQ_merge <- MCQ %>% \n  select(first_name,\n         surname,\n         id_number,\n         email_address,\n         quiz_mcq_summative_assessment_real,\n         quiz_mcq_summative_assessment_perc)"},{"path":"Echo_combine.html","id":"left_join","chapter":"7 Combining Echo360 data with other sources of data with tidyverse","heading":"7.3.1.1 left_join()","text":"first type left_join() retains observations data frame 1, meaning keep everything lecture_01_merge add information MCQ_merge. state email_address shared variable act key.important explore resulting data frame make sure everything expected.started 67 rows Echo360 data 243 rows MCQ data, wanted retain Echo360 observations, ended 67 rows.enter column act key, dplyr best identify two columns name. warn make sure double check using right variables. first example, variable also name makes things easier. However, might find information different name, can tell dplyr two columns want use joining key. See alternatives look like .","code":"\nlecture_01_MCQ_left <- left_join(lecture_01_merge,\n                                MCQ_merge, \n                                by = \"email_address\")\nglimpse(lecture_01_MCQ_left)## Rows: 67\n## Columns: 11\n## $ user_name                          <chr> \"Yaw, Collin\", \"Viaan, Jordan\", NA,…\n## $ email_address                      <chr> \"214488@university.ac.uk\", \"211717@…\n## $ total_views                        <dbl> 1, 1, 1, 3, 1, 1, 1, 2, 2, 2, 1, 2,…\n## $ total_view_time                    <time> 00:00:30, 00:01:00, 00:00:08, 00:0…\n## $ average_view_time                  <time> 00:00:30, 00:01:00, 00:00:08, 00:0…\n## $ last_viewed                        <chr> \"11/04/2022\", \"09/30/2022\", \"02/10/…\n## $ first_name                         <chr> \"Collin\", \"Jordan\", NA, NA, \"Hali\",…\n## $ surname                            <chr> \"Yaw\", \"Viaan\", NA, NA, \"Sunny\", \"T…\n## $ id_number                          <int> 214488, 211717, NA, NA, 278359, 272…\n## $ quiz_mcq_summative_assessment_real <int> 17, 22, NA, NA, 19, 19, 22, NA, 20,…\n## $ quiz_mcq_summative_assessment_perc <dbl> 77.27273, 100.00000, NA, NA, 86.363…\n# Warning which column it is joining by\nlecture_01_MCQ_left <- left_join(lecture_01_merge,\n                                MCQ_merge)## Joining with `by = join_by(email_address)`\n# Rename email\nlecture_01_email <- lecture_01_merge %>% \n  rename(email = email_address)\n\nlecture_01_MCQ_left <- left_join(lecture_01_email,\n                                MCQ_merge, \n                                by = c(\"email\" = \"email_address\"))"},{"path":"Echo_combine.html","id":"right_join","chapter":"7 Combining Echo360 data with other sources of data with tidyverse","heading":"7.3.1.2 right_join()","text":"contrast, right_join() retains observations data frame 2, MCQ data example.important explore resulting data frame make sure everything expected.started 67 rows Echo360 data 243 rows MCQ data, wanted retain MCQ observations, ended 243 rows.","code":"\nlecture_01_MCQ_right <- right_join(lecture_01_merge,\n                                  MCQ_merge, \n                                  by = \"email_address\")\nglimpse(lecture_01_MCQ_right)## Rows: 243\n## Columns: 11\n## $ user_name                          <chr> \"Yaw, Collin\", \"Viaan, Jordan\", \"Su…\n## $ email_address                      <chr> \"214488@university.ac.uk\", \"211717@…\n## $ total_views                        <dbl> 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 3,…\n## $ total_view_time                    <time> 00:00:30, 00:01:00, 00:01:00, 00:0…\n## $ average_view_time                  <time> 00:00:30, 00:01:00, 00:01:00, 00:0…\n## $ last_viewed                        <chr> \"11/04/2022\", \"09/30/2022\", \"10/08/…\n## $ first_name                         <chr> \"Collin\", \"Jordan\", \"Hali\", \"Franki…\n## $ surname                            <chr> \"Yaw\", \"Viaan\", \"Sunny\", \"Tzivia\", …\n## $ id_number                          <int> 214488, 211717, 278359, 272061, 275…\n## $ quiz_mcq_summative_assessment_real <int> 17, 22, 19, 19, 22, NA, 20, 21, 18,…\n## $ quiz_mcq_summative_assessment_perc <dbl> 77.27273, 100.00000, 86.36364, 86.3…"},{"path":"Echo_combine.html","id":"full_join","chapter":"7 Combining Echo360 data with other sources of data with tidyverse","heading":"7.3.1.3 full_join()","text":"Finally, right_join() retains observations data frame 1 data frame 2.important explore resulting data frame make sure everything expected.started 67 rows Echo360 data 243 rows MCQ data, wanted retain MCQ observations, ended 245 rows.summarise:left join retains nrow(lecture_01_MCQ_left) student email addresses listed Echo360 lecture_01_merge object;left join retains nrow(lecture_01_MCQ_left) student email addresses listed Echo360 lecture_01_merge object;Tthe right join retains nrow(lecture_01_MCQ_right) student email addresses listed MCQ_merge object;Tthe right join retains nrow(lecture_01_MCQ_right) student email addresses listed MCQ_merge object;full join retains nrow(lecture_01_MCQ_full) unique student email addresses listed lecture_01_merge MCQ_merge.full join retains nrow(lecture_01_MCQ_full) unique student email addresses listed lecture_01_merge MCQ_merge.inspection, two observations lecture_01_merge missing values (NA) email_address correspond two \"extra\" observations lecture_01_MCQ_full compared lecture_01_MCQ_right.","code":"\nlecture_01_MCQ_full <- full_join(lecture_01_merge,\n                                MCQ_merge, \n                                by = \"email_address\")\nglimpse(lecture_01_MCQ_full)## Rows: 245\n## Columns: 11\n## $ user_name                          <chr> \"Yaw, Collin\", \"Viaan, Jordan\", NA,…\n## $ email_address                      <chr> \"214488@university.ac.uk\", \"211717@…\n## $ total_views                        <dbl> 1, 1, 1, 3, 1, 1, 1, 2, 2, 2, 1, 2,…\n## $ total_view_time                    <time> 00:00:30, 00:01:00, 00:00:08, 00:0…\n## $ average_view_time                  <time> 00:00:30, 00:01:00, 00:00:08, 00:0…\n## $ last_viewed                        <chr> \"11/04/2022\", \"09/30/2022\", \"02/10/…\n## $ first_name                         <chr> \"Collin\", \"Jordan\", NA, NA, \"Hali\",…\n## $ surname                            <chr> \"Yaw\", \"Viaan\", NA, NA, \"Sunny\", \"T…\n## $ id_number                          <int> 214488, 211717, NA, NA, 278359, 272…\n## $ quiz_mcq_summative_assessment_real <int> 17, 22, NA, NA, 19, 19, 22, NA, 20,…\n## $ quiz_mcq_summative_assessment_perc <dbl> 77.27273, 100.00000, NA, NA, 86.363…"},{"path":"Echo_combine.html","id":"merging-data-where-each-student-may-appear-more-than-once","chapter":"7 Combining Echo360 data with other sources of data with tidyverse","heading":"7.4 Merging data where each student may appear more than once","text":"interested combining Echo360 data multiple lectures (thus students may appear ) together another data object students appear (checklist MCQ data described ), can still use left_join() function. Despite function including argument multiple \"handling rows x multiple matches y\", argument relevant scenario since Echo360 data x argument multiple rows student can merged another data source y argument (within student appears ) one ways shown previously.First, select variables interest data object, Echo360_data contains data first three lectures.students may appear , necessary include variable distinguishes repeated appearances. case variable media_name identifies lecture Echo360 data .Second, specify type 'join', case one.Third, check input output data objects expect.left join retains 145 observations listed Echo360_data_merge object combines (single) corresponding observations MCQ_merge.","code":"\nEcho360_data_merge <- Echo360_data %>% \n  select(user_name, \n         email_address,\n         media_name,\n         total_views,\n         total_view_time,\n         average_view_time,\n         last_viewed)\n\nMCQ_merge <- MCQ %>% \n  select(first_name,\n         surname,\n         id_number,\n         email_address,\n         quiz_mcq_summative_assessment_real,\n         quiz_mcq_summative_assessment_perc)\nEcho360_data_MCQ_left <- left_join(Echo360_data_merge,\n                                    MCQ_merge,\n                                    by = \"email_address\")\n# Dimensions of original echo 360 data\nglimpse(Echo360_data_merge)\n\n# Dimensions of echo 360 data joined with MCQ data \nglimpse(Echo360_data_MCQ_left)## Rows: 145\n## Columns: 7\n## $ user_name         <chr> \"Yaw, Collin\", \"Viaan, Jordan\", NA, NA, \"Sunny, Hali…\n## $ email_address     <chr> \"214488@university.ac.uk\", \"211717@university.ac.uk\"…\n## $ media_name        <chr> \"RM1 Lecture 1\", \"RM1 Lecture 1\", \"RM1 Lecture 1\", \"…\n## $ total_views       <dbl> 1, 1, 1, 3, 1, 1, 1, 2, 2, 2, 1, 2, 1, 3, 3, 1, 1, 5…\n## $ total_view_time   <time> 00:00:30, 00:01:00, 00:00:08, 00:04:30, 00:01:00, 0…\n## $ average_view_time <time> 00:00:30, 00:01:00, 00:00:08, 00:01:30, 00:01:00, 0…\n## $ last_viewed       <chr> \"11/04/2022\", \"09/30/2022\", \"02/10/2023\", \"10/21/202…\n## Rows: 145\n## Columns: 12\n## $ user_name                          <chr> \"Yaw, Collin\", \"Viaan, Jordan\", NA,…\n## $ email_address                      <chr> \"214488@university.ac.uk\", \"211717@…\n## $ media_name                         <chr> \"RM1 Lecture 1\", \"RM1 Lecture 1\", \"…\n## $ total_views                        <dbl> 1, 1, 1, 3, 1, 1, 1, 2, 2, 2, 1, 2,…\n## $ total_view_time                    <time> 00:00:30, 00:01:00, 00:00:08, 00:0…\n## $ average_view_time                  <time> 00:00:30, 00:01:00, 00:00:08, 00:0…\n## $ last_viewed                        <chr> \"11/04/2022\", \"09/30/2022\", \"02/10/…\n## $ first_name                         <chr> \"Collin\", \"Jordan\", NA, NA, \"Hali\",…\n## $ surname                            <chr> \"Yaw\", \"Viaan\", NA, NA, \"Sunny\", \"T…\n## $ id_number                          <int> 214488, 211717, NA, NA, 278359, 272…\n## $ quiz_mcq_summative_assessment_real <int> 17, 22, NA, NA, 19, 19, 22, NA, 20,…\n## $ quiz_mcq_summative_assessment_perc <dbl> 77.27273, 100.00000, NA, NA, 86.363…"},{"path":"Echo_combine.html","id":"visualizing-merged-data","chapter":"7 Combining Echo360 data with other sources of data with tidyverse","heading":"7.5 Visualizing merged data","text":"Merging data enables exploration video engagement (recorded Echo360 data) may related student performance. example, number times student views video content can combined performance assessment (contained MCQ data ).First, need brief wrangling summarise total number video views per student repeat joining process .now Echo360 student attainment data can use explore potential patterns. example, can look relationship two variables scatterplot. lot overlap, added little jitter data points easier see density. points look like extend beyond 100%.plot, look like clear relationship far video views 0 5, clustered top left show good performance 80%.previous chapter, might want identify data points interactive version plot, convert using plotly:purpose tutorials introduce inferential statistics, data available means can explore different ways. scatterplot suggest clear relationship visually total video views MCQ attainment, maybe subtle statistical relationship., can apply Spearman correlation since data may meet parametric assumptions given distribution variable.correlation coefficient slightly negative suggesting number video views increases, MCQ percentage tends decrease, relationship statistically significant.might inspire explore additional relationships variables see patterns worth investigating answer research questions might .","code":"\nstudent_data <- Echo360_data %>% \n  group_by(email_address) %>%\n  summarise(total_views = sum(total_views)) %>%\n  mutate(email_address = factor(email_address)) %>% \n  ungroup()\n\nstudent_MCQ <- left_join(student_data,\n                         MCQ_merge,\n                         by = \"email_address\") %>% \n  rename(MCQ_perc = quiz_mcq_summative_assessment_perc) # Rename super long name\nfig.no_viewed.mcq <- student_MCQ %>% \n  ggplot(aes(x = total_views, y = MCQ_perc)) +\n  geom_jitter() + \n  labs(title= \"Number of Video Views vs. Percentage Score in an MCQ\") + \n  ylab(\"Percentage Score in MCQ (%)\") +\n  xlab(\"No. of Video Views\") +\n  theme_bw()\n\nfig.no_viewed.mcq\nggplotly(fig.no_viewed.mcq)\ncor.test(student_MCQ$total_views, \n         student_MCQ$MCQ_perc, \n         method = \"spearman\")## Warning in cor.test.default(student_MCQ$total_views, student_MCQ$MCQ_perc, :\n## Cannot compute exact p-value with ties## \n##  Spearman's rank correlation rho\n## \n## data:  student_MCQ$total_views and student_MCQ$MCQ_perc\n## S = 133575, p-value = 0.2004\n## alternative hypothesis: true rho is not equal to 0\n## sample estimates:\n##        rho \n## -0.1370056"},{"path":"installing-r.html","id":"installing-r","chapter":"A Installing R","heading":"A Installing R","text":"Installing R RStudio usually straightforward. sections explain helpful YouTube video . run serious difficulties (example admin rights machine), purposes workshop recommend using RStudio Cloud time trouble-shoot complex installation issues.","code":""},{"path":"installing-r.html","id":"installing-base-r","chapter":"A Installing R","heading":"A.1 Installing Base R","text":"Install base R. Choose download link operating system (Linux, Mac OS X, Windows).Mac, install latest release newest R-x.x.x.pkg link (legacy version older operating system). may also need install XQuartz able use visualisation packages.installing Windows version, choose \"base\" subdirectory click download link top page.using Linux, choose specific operating system follow installation instructions.","code":""},{"path":"installing-r.html","id":"installing-rstudio","chapter":"A Installing R","heading":"A.2 Installing RStudio","text":"Go rstudio.com download RStudio Desktop (Open Source License) version operating system list titled Installers Supported Platforms.","code":""},{"path":"installing-r.html","id":"installing-rtools","chapter":"A Installing R","heading":"A.3 Installing RTools","text":"using Windows, install R, also install RTools; use \"recommended\" version highlighted near top list. RTools used installing loading packages. can get started without installing RTools, problems installing loading packages, first thing try.RTools require put \"PATH\". instructions can seem bit vague - easiest way open RStudio, run code console:done , restart R clicking Session - Restart R run code console give path RTools installation:","code":"\nwrite('PATH=\"${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}\"', file = \"~/.Renviron\", append = TRUE)\nSys.which(\"make\")##                               make \n## \"C:\\\\rtools40\\\\usr\\\\bin\\\\make.exe\""},{"path":"installing-r.html","id":"rstudio-settings","chapter":"A Installing R","heading":"A.4 RStudio Settings","text":"settings fix immediately updating RStudio. Go Global Options... Tools menu (⌘,), General tab, uncheck box says Restore .RData workspace startup. keep things around workspace, things get messy, unexpected things happen. always start clear workspace. also means never want save workspace exit, set Never. thing want save scripts.may also want change appearance code. Different fonts themes can sometimes help visual difficulties dyslexia.\nFigure .1: RStudio General Appearance settings\nmay also want change settings Code tab. example, Lisa DeBruine prefers two spaces instead tabs code likes able see whitespace characters. matter personal preference.\nFigure .2: RStudio Code settings\n","code":""},{"path":"installing-r.html","id":"installing-latex","chapter":"A Installing R","heading":"A.5 Installing LaTeX","text":"can install LaTeX typesetting system produce PDF reports RStudio. Without additional installation, able produce reports HTML PDF. generate PDF reports, additionally need install tinytex (Xie, 2023) run following code:","code":"\n# run this in the console\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()"},{"path":"installing-r.html","id":"updating-r","chapter":"A Installing R","heading":"A.6 Updating R, RStudio, and packages","text":"time--time, updated versions R, RStudio, packages use (e.g., ggplot) become available. Remember separate, different process come different considerations. recommend updating latest version start new project. definitely recommend updating middle project middle semester bring advice based personal experience pain.","code":""},{"path":"installing-r.html","id":"updating-rstudio","chapter":"A Installing R","heading":"A.7 Updating RStudio","text":"RStudio easiest component update. Typically, updates RStudio affect code, instead add new features, like spell-check upgrades RStudio can . usually little downside updating RStudio easy .Click Help - Check updates\nFigure .3: Updating RStudio\nupdate available, prompt download can install usual.","code":""},{"path":"installing-r.html","id":"updating-r-1","chapter":"A Installing R","heading":"A.8 Updating R","text":"Finally, may also wish update R . key thing aware update R, just download latest version website, lose packages.","code":""},{"path":"installing-r.html","id":"windows","chapter":"A Installing R","heading":"A.8.1 Windows","text":"easiest way update R Windows cause huge headache use installr package. use updateR() function, series dialogue boxes appear. fairly self-explanatory full step--step guide available use installr, important bit select \"Yes\" asked like copy packages older version R.","code":"\n# Install the installr package\ninstall.packages(\"installr\")\n\n# Run the update function\ninstallR::updateR()"},{"path":"installing-r.html","id":"mac","chapter":"A Installing R","heading":"A.8.2 Mac","text":"Mac, can use updateR package. need install GitHub. asked type system password (use log computer) console pane. relevant, ask want restore packages new major version.","code":"\n# install from github\ndevtools::install_github(\"AndreaCirilloAC/updateR\")\n\n# update your R version, you will need your system password\nupdateR::updateR()"},{"path":"installing-r.html","id":"updating-packages","chapter":"A Installing R","heading":"A.9 Updating packages","text":"completely new R installed packages yet, section make great deal sense, just remember can come back future.Package developers occasionally release updates packages. typically add new functions package, fix amend existing functions. aware package updates may cause previous code stop working. tend happen minor updates packages, occasionally major updates, can serious issues developer made fundamental changes code works. reason, recommend updating packages beginning academic year (semester) - assessment deadline just case!update individual package, easiest way use install.packages() function, always installs recent version package.update multiple packages, indeed packages, RStudio provides helpful tools. Click Tools - Check Package Updates. dialogue box appear can select packages wish update. aware select packages, may take time unable use R whilst process completes.\nFigure .4: Updating packages RStudio\n","code":"\ninstall.packages(\"tidyverse\")"},{"path":"installing-r.html","id":"package-install-troubleshooting","chapter":"A Installing R","heading":"A.10 Troubleshooting","text":"Occasionally, might problem packages seemingly refuse update install. Emily, rlang vctrs cause end trouble. packages likely every explicitly load, required beneath surface R things like knit Markdown files etc.","code":""},{"path":"installing-r.html","id":"non-zero-exit-status","chapter":"A Installing R","heading":"A.10.1 Non-zero exit status","text":"try update package get error message says something like Warning install.packages : installation package ‘vctrs’ non-zero exit status perhaps Error loadNamespace(, c(lib.loc, .libPaths()), versionCheck = vI[[]]) :  namespace 'rlang' 0.4.9 loaded, >= 0.4.10 required one solution found manually uninstall package, restart R, install package new, rather trying update existing version. installr package also useful function uninstalling packages.","code":"\n# Load installr\nlibrary(installr)\n\n# Uninstall the problem package\nuninstall.packages(\"package_name\")\n\n# Then restart R using session - restart R\n# Then install the package fresh\n\ninstall.packages(\"package\")"},{"path":"installing-r.html","id":"cannot-open-file","chapter":"A Installing R","heading":"A.10.2 Cannot open file","text":"may get following error trying install packages :Error install packages : open file 'C:/.....': Permission deniedThis usually indicates permissions problem writing default library (folder packages kept ). Sometimes means need install R RStudio administrator run administrator.One fix may change library location using following code (check \"C:/Program Files/R\" version instead \"R-3.5.2\"):works can install packages, set library path permanently:Install usethis packageRun usethis::edit_r_profile() console; open blank filePaste file (version ): .libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))Save close fileRestart R changes take effectThe code .Rprofile now run every time start R.","code":"\n# change the library path\n.libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))"},{"path":"symbols.html","id":"symbols","chapter":"B Symbols","heading":"B Symbols","text":"\nFigure B.1: Image James Chapman/Soundimals\n","code":""},{"path":"conventions.html","id":"conventions","chapter":"C Conventions","heading":"C Conventions","text":"book use following conventions:Generic code: list(number = 1, letter = \"\")Highlighted code: dplyr::slice_max()File paths: data/sales.csvR Packages: tidyverseFunctions: paste()Strings: \"psyTeachR\"Numbers: 100, 3.14Logical values: TRUE, FALSEGlossary items: ordinalCitations: Wickham (2023)Internal links: Chapter 1External links: R Data ScienceMenu/interface options: New File...","code":""},{"path":"conventions.html","id":"webexercises","chapter":"C Conventions","heading":"C.1 Webexercises","text":"See webexercises details use materials.Type integer: going learn lot: TRUEFALSEWhat p-value?\n\nprobability null hypothesis truethe probability observed (extreme) data, assumption null-hypothesis truethe probability making error conclusion\nfound hidden text!","code":"\nprint(\"You found some hidden code!\")## [1] \"You found some hidden code!\""},{"path":"conventions.html","id":"alert-boxes","chapter":"C Conventions","heading":"C.2 Alert boxes","text":"Informational asides.Notes warn something.Notes things cause serious errors.Try .","code":""},{"path":"conventions.html","id":"code-chunks","chapter":"C Conventions","heading":"C.3 Code Chunks","text":"","code":"\n# code chunks\npaste(\"Applied\", \"Data\", \"Skills\", 1, sep = \" \")## [1] \"Applied Data Skills 1\"```{r setup, message = FALSE}\n# code chunks with visible r headers\nlibrary(tidyverse)```"},{"path":"conventions.html","id":"glossary","chapter":"C Conventions","heading":"C.4 Glossary","text":"","code":""},{"path":"license.html","id":"license","chapter":"License","heading":"License","text":"book licensed Creative Commons Attribution-ShareAlike 4.0 International License (CC--SA 4.0). free share adapt book. must give appropriate credit (DeBruine, 2021), provide link license, indicate changes made. adapt material, must distribute contributions license original.","code":""},{"path":"references-1.html","id":"references-1","chapter":"References","heading":"References","text":"","code":""}]
